{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qATAyIXprzdx",
    "outputId": "1cb1c37f-80e8-4bdc-a72a-7cfa8142e460"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'cpsc-522-project' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/sbc806/cpsc-522-project.git"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import sys as sys\n",
    "\n",
    "sys.path.append(\"cpsc-522-project\")"
   ],
   "metadata": {
    "id": "jSogYSRK1UpO"
   },
   "execution_count": 45,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install scanpy"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vCtv8Kp51bdF",
    "outputId": "8f732c3a-4f4a-4dcc-9dc1-47f8a79ee4d1"
   },
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scanpy in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (1.9.3)\n",
      "Requirement already satisfied: scipy>=1.4 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (1.10.0)\n",
      "Requirement already satisfied: anndata>=0.7.4 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (0.9.1)\n",
      "Requirement already satisfied: numba>=0.41.0 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (0.56.4)\n",
      "Requirement already satisfied: umap-learn>=0.3.10 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (0.5.3)\n",
      "Requirement already satisfied: patsy in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (0.5.3)\n",
      "Requirement already satisfied: statsmodels>=0.10.0rc2 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (0.13.5)\n",
      "Requirement already satisfied: pandas>=1.0 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (1.5.3)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (23.0)\n",
      "Requirement already satisfied: seaborn in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (0.12.2)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (4.64.1)\n",
      "Requirement already satisfied: h5py>=3 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (3.8.0)\n",
      "Requirement already satisfied: networkx>=2.3 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (3.1)\n",
      "Requirement already satisfied: session-info in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (1.0.0)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (1.2.0)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (1.2.0)\n",
      "Requirement already satisfied: natsort in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (8.3.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (1.23.5)\n",
      "Requirement already satisfied: matplotlib>=3.4 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (3.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from matplotlib>=3.4->scanpy) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from matplotlib>=3.4->scanpy) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from matplotlib>=3.4->scanpy) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from matplotlib>=3.4->scanpy) (9.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from matplotlib>=3.4->scanpy) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from matplotlib>=3.4->scanpy) (1.0.7)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from matplotlib>=3.4->scanpy) (3.0.9)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from numba>=0.41.0->scanpy) (66.0.0)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from numba>=0.41.0->scanpy) (0.39.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from pandas>=1.0->scanpy) (2022.7.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scikit-learn>=0.22->scanpy) (3.1.0)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from patsy->scanpy) (1.16.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from umap-learn>=0.3.10->scanpy) (0.5.8)\n",
      "Requirement already satisfied: stdlib-list in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from session-info->scanpy) (0.8.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from tqdm->scanpy) (0.4.6)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip3 install bbknn"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QOPwKn5w1dmV",
    "outputId": "fc7a5cec-1617-46c4-9ca0-28e9964e183f"
   },
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bbknn in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: pynndescent in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from bbknn) (0.5.8)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from bbknn) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from bbknn) (1.2.0)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from bbknn) (1.10.0)\n",
      "Requirement already satisfied: umap-learn in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from bbknn) (0.5.3)\n",
      "Requirement already satisfied: annoy in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from bbknn) (1.17.2)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from bbknn) (23.0)\n",
      "Requirement already satisfied: Cython in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from bbknn) (0.29.33)\n",
      "Requirement already satisfied: llvmlite>=0.30 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from pynndescent->bbknn) (0.39.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from pynndescent->bbknn) (1.2.0)\n",
      "Requirement already satisfied: numba>=0.51.2 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from pynndescent->bbknn) (0.56.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scikit-learn->bbknn) (3.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from umap-learn->bbknn) (4.64.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from numba>=0.51.2->pynndescent->bbknn) (66.0.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from tqdm->umap-learn->bbknn) (0.4.6)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import os as os\n",
    "import scanpy as sc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from collections import defaultdict\n",
    "\n",
    "from hyperspherical_vae.distributions import VonMisesFisher\n",
    "from hyperspherical_vae.distributions import HypersphericalUniform"
   ],
   "metadata": {
    "id": "AGicNh_U1f-d"
   },
   "execution_count": 48,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sc.settings.verbosity = 3  # verbosity: errors (0), warnings (1), info (2), hints (3)\n",
    "sc.logging.print_header()\n",
    "sc.settings.set_figure_params(dpi=80, facecolor='white')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gvlvkqgd1iGv",
    "outputId": "fd7c95df-b67a-4c64-e57d-6584f3dc0d57"
   },
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.23.5 scipy==1.10.0 pandas==1.5.3 scikit-learn==1.2.0 statsmodels==0.13.5 pynndescent==0.5.8\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def read_data(filename):\n",
    "    adata = sc.read_h5ad(filename)\n",
    "    return adata"
   ],
   "metadata": {
    "id": "JiFu2bQd1kNH"
   },
   "execution_count": 50,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "file_path = \"./data\"\n",
    "# file_path = \"./drive/MyDrive/immune_cell_dataset/\"\n",
    "# data can be myeloid, b_cells, or t_cells\n",
    "data = \"myeloid\"\n",
    "# file_path = os.path.join(file_path, data)\n",
    "if data == \"myeloid\":\n",
    "    filename = \"myeloid.h5ad\"\n",
    "elif data == \"b_cells\":\n",
    "    filename = \"b-cells.h5ad\"\n",
    "else:\n",
    "    filename = \"t-cells.h5ad\"\n",
    "file_path = os.path.join(file_path, filename)\n",
    "# file_path = os.path.join(file_path, filename)\n",
    "\n",
    "adata = read_data(file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting highly variable genes\n",
      "    finished (0:00:02)\n",
      "--> added\n",
      "    'highly_variable', boolean vector (adata.var)\n",
      "    'means', float vector (adata.var)\n",
      "    'dispersions', float vector (adata.var)\n",
      "    'dispersions_norm', float vector (adata.var)\n",
      "(51552, 1816)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\ntk\\lib\\site-packages\\scanpy\\preprocessing\\_simple.py:843: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... as `zero_center=True`, sparse input is densified and may lead to large memory consumption\n"
     ]
    }
   ],
   "source": [
    "# Data already normalized and log-transformed\n",
    "# Select highly variable genes\n",
    "sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5)\n",
    "adata = adata[:, adata.var.highly_variable]\n",
    "print(adata.X.shape)\n",
    "# Scale to unit variance and zero mean\n",
    "sc.pp.scale(adata)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get rid of batch effects"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing ridge regression\n",
      "\tfinished: `.X` now features regression residuals\n",
      "\t`.layers['X_explained']` stores the expression explained by the technical effect (0:00:00)\n"
     ]
    }
   ],
   "source": [
    "import bbknn\n",
    "\n",
    "bbknn.ridge_regression(adata, batch_key=['Chemistry'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "\n",
    "# noinspection PyUnresolvedReferences\n",
    "# noinspection PyCallingNonCallable\n",
    "class ProductSpaceVAE(torch.nn.Module):\n",
    "\n",
    "    # def __init__(self, h_dims, z_dims, input_size=[1, 28, 28], input_type = 'binary', distribution='normal',\n",
    "    # r=None, encode_type='mlp', decode_type='mlp', device='cpu', flags=None):\n",
    "    def __init__(self, z_dims, n_gene, distribution='normal',\n",
    "                 r=None, encoder_layer=None, decoder_layer=None, activation=F.relu,\n",
    "                 device='cpu', flags=None):\n",
    "        \"\"\"\n",
    "        ModelVAE initializer\n",
    "        # :param in_dim: dimension of input\n",
    "        :param n_gene: dimension of input\n",
    "        # :param h_dims: dimension of the hidden layers, list\n",
    "        :param z_dims: dimensions of the latent representation, list\n",
    "        :param distribution: string either `normal` or `vmf`, indicates which distribution to use\n",
    "        :param r: radii scalars, list\n",
    "        :encoder_layer: a list with the units of each layer for the encoder\n",
    "        :decoder_layer: a list with units of each layer for the decoder\n",
    "        :param device: device to use\n",
    "        \"\"\"\n",
    "        super(ProductSpaceVAE, self).__init__()\n",
    "\n",
    "        self.flags = flags\n",
    "        self.name = 'productspace'\n",
    "        self.epochs, self.num_restarts = 0, 0\n",
    "        # self.input_size, self.distribution, self.device = input_size, distribution, device\n",
    "        self.n_gene, self.distribution, self.device = n_gene, distribution, device\n",
    "        # self.encode_type, self.decode_type = encode_type, decode_type\n",
    "        self.activation = activation\n",
    "\n",
    "        self.z_dims = np.sort(np.asarray(z_dims))\n",
    "        self.z_unique, self.z_counts = np.unique(self.z_dims, return_counts=True)\n",
    "        self.z_u_idx = [np.where(self.z_dims == u)[0] for u in self.z_unique]\n",
    "\n",
    "        self.r = torch.ones(len(z_dims), device=device) if r is None else r  # not used yet\n",
    "\n",
    "        # self.encoder, self.fc_means, self.fc_vars = create_encoder(input_size, input_type, self.z_dims, h_dims,\n",
    "        # distribution, encode_type, flags)\n",
    "        if encoder_layer is None:\n",
    "            encoder_layer = [128, 64, 32]\n",
    "        if decoder_layer is None:\n",
    "            decoder_layer = [32, 128]\n",
    "\n",
    "        # Output of the encoder\n",
    "        h_last = encoder_layer[-1]\n",
    "        self.fc_means = nn.ModuleList([nn.Linear(h_last, z_dim) for z_dim in z_dims])\n",
    "        self.fc_vars = nn.ModuleList([nn.Sequential(nn.Linear(h_last, (1 if distribution == 'vmf' else z_dim)),\n",
    "                                                    nn.Softplus(), nn.Hardtanh(min_val=0.01, max_val=7.))\n",
    "                                      for z_dim in z_dims])\n",
    "        # Output of decoder\n",
    "        self.mu_layer = nn.Linear(decoder_layer[-1], n_gene)\n",
    "        self.var_layer = nn.Linear(decoder_layer[-1], n_gene)\n",
    "\n",
    "        # self.decoder = create_decoder(input_size, input_type, z_dims, h_dims, decode_type)\n",
    "\n",
    "        # Create layers for encoder\n",
    "        self.encoder_layer = [nn.Linear(n_gene, encoder_layer[0])]\n",
    "        # BatchNorm\n",
    "        # self.encoder_batch_norm = [nn.BatchNorm1d(encoder_layer[0])]\n",
    "        for i in range(1, len(encoder_layer)):\n",
    "            self.encoder_layer.append(nn.Linear(encoder_layer[i - 1], encoder_layer[i]))\n",
    "            # BatchNorm\n",
    "            # self.encoder_batch_norm.append(nn.BatchNorm1d(encoder_layer[i]))\n",
    "        self.encoder_layer = nn.ModuleList(self.encoder_layer)\n",
    "        # BatchNorm\n",
    "        # self.encoder_batch_norm = nn.ModuleList(self.encoder_batch_norm)\n",
    "\n",
    "        # self.decoder_layer = [nn.Linear(z_dim, decoder_layer[0])]\n",
    "        self.decoder_layer = [nn.Linear(sum(z_dims), decoder_layer[0])]\n",
    "        # BatchNorm\n",
    "        # self.decoder_batch_norm = [nn.BatchNorm1d(decoder_layer[0])]\n",
    "        for i in range(1, len(decoder_layer)):\n",
    "            self.decoder_layer.append(nn.Linear(decoder_layer[i - 1], decoder_layer[i]))\n",
    "            # BatchNorm\n",
    "            # self.decoder_batch_norm.append(nn.BatchNorm1d(decoder_layer[i]))\n",
    "        self.decoder_layer = nn.ModuleList(self.decoder_layer)\n",
    "        # BatchNorm\n",
    "        # self.decoder_batch_norm = nn.ModuleList(self.decoder_batch_norm)\n",
    "\n",
    "    @staticmethod\n",
    "    def _print(x):\n",
    "        print('\\n')\n",
    "        print(x)\n",
    "        print('\\n')\n",
    "\n",
    "    def encode(self, x):\n",
    "        # x = self.encoder_mlp(x)\n",
    "\n",
    "        # if self.encode_type == 'cnn':\n",
    "        # x = x.reshape(x.size(0), *self.input_size)\n",
    "\n",
    "        # h = self.encoder(x)\n",
    "        # h = h.view(h.size(0), -1)\n",
    "\n",
    "        # regularizer = torch.\n",
    "\n",
    "        h = self.encoder_layer[0](x)\n",
    "        h = self.activation(h)\n",
    "        # Add in batch normalization here\n",
    "        # h = self.encoder_batch_norm[0](h)\n",
    "        for i in range(1, len(self.encoder_layer)):\n",
    "            h = self.encoder_layer[i](h)\n",
    "            h = self.activation(h)\n",
    "            # Add in batch normalization here\n",
    "            # h = self.encoder_batch_norm[i](h)\n",
    "\n",
    "        if self.distribution == 'normal':\n",
    "            # compute means and stds of the normal distributions\n",
    "            z_means = [f(h) for f in self.fc_means]\n",
    "            z_vars = [f(h) for f in self.fc_vars]\n",
    "        elif self.distribution == 'vmf':\n",
    "            # compute means and concentrations of the von Mises-Fishers\n",
    "            z_means_unnormalized = [f(h) for f in self.fc_means]\n",
    "            z_means = [zmu / zmu.norm(dim=-1, keepdim=True) for zmu in z_means_unnormalized]\n",
    "            z_vars = [(f(h) + 1.) for f in self.fc_vars]  # the `+ 1` prevents collapsing behaviors\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "\n",
    "        return z_means, z_vars\n",
    "\n",
    "    def decode(self, z):\n",
    "        # if self.decode_type == 'cnn':\n",
    "        # z = z.view(z.size(0), sum(self.z_dims), 1, 1)\n",
    "\n",
    "        # x_recon = self.decoder(z)\n",
    "\n",
    "        # return x_recon.view(x_recon.size(0), -1)\n",
    "        # l2 regularization goes here\n",
    "        h = self.decoder_layer[0](z)\n",
    "        h = self.activation(h)\n",
    "        # Add in batch normalization here\n",
    "        # h = self.decoder_batch_norm[0](h)\n",
    "        for i in range(1, len(self.decoder_layer)):\n",
    "            h = self.decoder_layer[i](h)\n",
    "            h = self.activation(h)\n",
    "            # Add in batch normalization here\n",
    "            # h = self.decoder_batch_norm[i](h)\n",
    "\n",
    "        mu = self.mu_layer(h)\n",
    "        sigma_square = F.softplus(self.var_layer(h))\n",
    "        sigma_sqare = torch.clip(sigma_square, 1e-6, 1e10)\n",
    "        return mu, sigma_square\n",
    "\n",
    "    def reparameterize(self, z_means, z_vars):\n",
    "\n",
    "        # since z is sorted, we take the min index, and the max index, to slice the list of z_means, z_vars\n",
    "        # this is done to not have convert to numpy array\n",
    "        gather_zvs = [(torch.cat(z_means[min(u_idx):max(u_idx) + 1], 0),\n",
    "                       torch.cat(z_vars[min(u_idx):max(u_idx) + 1], 0))\n",
    "                      for u_idx in self.z_u_idx]\n",
    "\n",
    "        if self.distribution == 'normal':\n",
    "            # for each pair of z_mean, z_var, we make a distribution (sampling) object\n",
    "            q_zs_sample = [torch.distributions.normal.Normal(z_mean, z_var) for (z_mean, z_var) in gather_zvs]\n",
    "\n",
    "            q_zs = [torch.distributions.normal.Normal(z_mean, z_var) for z_mean, z_var in zip(z_means, z_vars)]\n",
    "            p_zs = [torch.distributions.normal.Normal(torch.zeros_like(z_mean), torch.ones_like(z_var)) for\n",
    "                    z_mean, z_var in zip(z_means, z_vars)]\n",
    "\n",
    "        elif self.distribution == 'vmf':\n",
    "            # for each pair of z_mean, z_var, we make a distribution (sampling) object\n",
    "            q_zs_sample = [VonMisesFisher(z_mean, z_var) for (z_mean, z_var) in gather_zvs]\n",
    "\n",
    "            q_zs = [VonMisesFisher(z_mean, z_var) for z_mean, z_var in zip(z_means, z_vars)]\n",
    "            p_zs = [HypersphericalUniform(z_dim - 1, device=self.device) for z_dim in self.z_dims]\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "\n",
    "        return q_zs, p_zs, q_zs_sample\n",
    "\n",
    "    # def loss(self, q_zs, p_zs, x_mb, x_mb_recon):\n",
    "    def kl_divergence(self, q_zs, p_zs):\n",
    "        # if self.flags.loss_function == 'bce':\n",
    "        # lf = nn.BCEWithLogitsLoss(reduction='none')\n",
    "        # elif self.flags.loss_function == 'mse':\n",
    "        # lf = nn.MSELoss(reduction='none')\n",
    "        # else:\n",
    "        # raise NotImplemented\n",
    "        # loss_recon = lf(x_mb_recon, x_mb.reshape(x_mb.size(0), -1)).sum(-1).mean()\n",
    "\n",
    "        if self.distribution == 'normal':\n",
    "            loss_kl = torch.stack([torch.distributions.kl.kl_divergence(q_z, p_z).sum(-1) for\n",
    "                                   q_z, p_z in zip(q_zs, p_zs)], dim=-1).sum(-1).mean()\n",
    "        elif self.distribution == 'vmf':\n",
    "            loss_kl = torch.stack([torch.distributions.kl.kl_divergence(q_z, p_z) for\n",
    "                                   q_z, p_z in zip(q_zs, p_zs)], dim=-1).sum(-1).mean()\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "\n",
    "        # return loss_recon, loss_kl, None\n",
    "        return loss_kl\n",
    "\n",
    "    # def log_likelihood(self, x, n=10):\n",
    "    # \"\"\"\n",
    "    # :param x: e.g. MNIST data flattened\n",
    "    # :param n: number of MC samples\n",
    "    # :return: MC estimate of log-likelihood\n",
    "    # \"\"\"\n",
    "\n",
    "    # z_means, z_vars = self.encode(x.reshape(x.size(0), -1))\n",
    "    # q_zs, p_zs, _, = self.reparameterize(z_means, z_vars)\n",
    "    # z_parts = [q_z.rsample(torch.Size([n])) for q_z in q_zs]\n",
    "    # z = torch.cat(z_parts, dim=-1).reshape(n*x.size(0), -1)\n",
    "\n",
    "    # x_mb_recon = self.decode(z)\n",
    "    # x_mb_recon = x_mb_recon.reshape(n, x.size(0), -1)\n",
    "\n",
    "    # if self.flags.loss_function == 'bce':\n",
    "    # lf = nn.BCEWithLogitsLoss(reduction='none')\n",
    "    # elif self.flags.loss_function == 'mse':\n",
    "    # lf = nn.MSELoss(reduction='none')\n",
    "    # else:\n",
    "    # raise NotImplemented\n",
    "    # log_p_x_z = -lf(x_mb_recon, x.reshape(x.size(0), -1).repeat((n, 1, 1))).sum(-1)\n",
    "\n",
    "    # if self.distribution == 'normal':\n",
    "    # log_p_z = torch.stack([p_z.log_prob(z__).sum(-1) for p_z, z__ in zip(p_zs, z_parts)], dim=-1).sum(-1)\n",
    "    # log_q_z_x = torch.stack([q_z.log_prob(z__).sum(-1) for q_z, z__ in zip(q_zs, z_parts)], dim=-1).sum(-1)\n",
    "    # elif self.distribution == 'vmf':\n",
    "    # log_p_z = torch.stack([p_z.log_prob(z__) for p_z, z__ in zip(p_zs, z_parts)], dim=-1).sum(-1)\n",
    "    # log_q_z_x = torch.stack([q_z.log_prob(z__) for q_z, z__ in zip(q_zs, z_parts)], dim=-1).sum(-1)\n",
    "    # else:\n",
    "    # raise NotImplementedError\n",
    "\n",
    "    # return ((log_p_x_z + log_p_z.to(self.device) - log_q_z_x).t().logsumexp(-1) - np.log(n)).mean()\n",
    "\n",
    "    def forward(self, x, n=None):\n",
    "\n",
    "        z_means, z_vars = self.encode(x)\n",
    "        if torch.isnan(z_means[0]).sum() > 0 or torch.isnan(z_vars[0]).sum() > 0:\n",
    "            return (None, None), None, None\n",
    "\n",
    "        q_zs, p_zs, q_zs_sample = self.reparameterize(z_means, z_vars)\n",
    "\n",
    "        # sample z1, z2, .., zk and concatenate\n",
    "        # z = torch.cat([q_z.rsample(torch.Size() if n is None else torch.Size([n])) for q_z in q_zs], dim=-1)  # slow\n",
    "        z = torch.cat([torch.cat(torch.chunk(q_z.rsample(), int(c), dim=0), dim=-1)\n",
    "                       for q_z, c in zip(q_zs_sample, self.z_counts)], dim=-1)\n",
    "        # z_parts = list(torch.split(z, tuple(self.z_unique.repeat(self.z_counts)), -1))\n",
    "        mu, sigma_square = self.decode(z)\n",
    "\n",
    "        return (q_zs, p_zs), z, mu, sigma_square"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "def log_likelihood(model, x, n=10):\n",
    "    z_mean, z_var = model.encode(x)\n",
    "    q_zs, p_zs, q_zs_sample = model.reparameterize(z_mean, z_var)\n",
    "    z = q_z.rsample(torch.Size([n]))\n",
    "    # Need to add a line here like z = torch.cat() in the forward_pass(self, x, n=None)\n",
    "    mu_, sigma_square_ = model.decode(z)\n",
    "\n",
    "    # In scPhere used tf.reduce_mean()\n",
    "    return torch.mean(log_likelihood_student(x, mu_, sigma_square_, df=2.0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "def log_likelihood_student(x, mu, sigma_square, df=2.0):\n",
    "    sigma = torch.sqrt(sigma_square)\n",
    "\n",
    "    dist = torch.distributions.studentT.StudentT(df=df,\n",
    "                                                 loc=mu,\n",
    "                                                 scale=sigma)\n",
    "\n",
    "    # return tf.reduce_sum(dist.log_prob(x), reduction_indices=1)\n",
    "    return torch.mean(dist.log_prob(x), dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "def train(model, optimizer, device='cuda'):\n",
    "    # for i, (x_mb, y_mb) in enumerate(train_loader):\n",
    "    for i in range(0, X.shape[0], 128):\n",
    "        # for x in X:\n",
    "        if i + 128 < X.shape[0]:\n",
    "            x = X[i: i + 128, :].to(device)\n",
    "        else:\n",
    "            x = X[i:, :].to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # dynamic binarization\n",
    "        # x_mb = (x_mb > torch.distributions.Uniform(0, 1).sample(x_mb.shape)).float()\n",
    "\n",
    "        # _, (q_z, p_z), _, x_mb_ = model(x_mb.reshape(-1, 784))\n",
    "        (q_zs, p_zs), z, mu, sigma_square = model(x)\n",
    "\n",
    "        # loss_recon = nn.BCEWithLogitsLoss(reduction='none')(x_reconstructed, x).sum(-1).mean()\n",
    "        # loss_recon = log_likelihood(x, z_mean, z_var)\n",
    "        # library_size = torch.sum(x, dim=1, keepdim=True)\n",
    "        # mu = mu * library_size\n",
    "        # loss_recon = log_likelihood_nb(\n",
    "        # x,\n",
    "        # mu,\n",
    "        # sigma_square,\n",
    "        # ).sum(-1).mean()\n",
    "        # loss_recon = torch.mean(log_likelihood_nb(x, mu, sigma_square, eps=1e-10))\n",
    "        loss_recon = torch.mean(log_likelihood_student(x, mu, sigma_square, df=5.0))\n",
    "\n",
    "        if model.distribution == 'normal':\n",
    "            loss_KL = torch.distributions.kl.kl_divergence(q_z, p_z).sum(-1).mean()\n",
    "        elif model.distribution == 'vmf':\n",
    "            # loss_KL = torch.distributions.kl.kl_divergence(q_z, p_z).mean()\n",
    "            loss_KL = model.kl_divergence(q_zs, p_zs)\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "        loss = loss_recon - loss_KL\n",
    "        # loss = loss_recon + loss_KL\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "def test(model, optimizer, device='cuda'):\n",
    "    print_ = defaultdict(list)\n",
    "    # for x_mb, y_mb in test_loader:\n",
    "    for i in range(0, X.shape[0], 128):\n",
    "        if i + 128 < X.shape[0]:\n",
    "            x = X[i:i + 128, :].to(device)\n",
    "        else:\n",
    "            x = X[i:, :].to(device)\n",
    "\n",
    "        # dynamic binarization\n",
    "        # x_mb = (x_mb > torch.distributions.Uniform(0, 1).sample(x_mb.shape)).float()\n",
    "\n",
    "        # _, (q_z, p_z), _, x_mb_ = model(x_mb.reshape(-1, 784))\n",
    "        (q_zs, p_zs), z, mu_, sigma_square_ = model(x)\n",
    "\n",
    "        # print_['recon loss'].append(float(nn.BCEWithLogitsLoss(reduction='none')(x_mb_,\n",
    "        # x_mb.reshape(-1, 784)).sum(-1).mean().data))\n",
    "        # library_size = torch.sum(x, dim=1, keepdim=True)\n",
    "        # mu_ = mu_* library_size\n",
    "        # print_['recon loss'].append(float(torch.mean(log_likelihood_nb(x, mu_, sigma_square_)).data))\n",
    "        print_['recon loss'].append(float(torch.mean(log_likelihood_student(x, mu_, sigma_square_, df=5.0)).data))\n",
    "\n",
    "        if model.distribution == 'normal':\n",
    "            print_['KL'].append(float(torch.distributions.kl.kl_divergence(q_z, p_z).sum(-1).mean().data))\n",
    "        elif model.distribution == 'vmf':\n",
    "            # print_['KL'].append(float(torch.distributions.kl.kl_divergence(q_z, p_z).mean().data))\n",
    "            print_['KL'].append(float(model.kl_divergence(q_zs, p_zs).mean().data))\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "\n",
    "        print_['ELBO'].append(print_['recon loss'][-1] - print_['KL'][-1])\n",
    "        # print_['LL'].append(float(log_likelihood(model, x).data))\n",
    "\n",
    "    print({k: np.mean(v) for k, v in print_.items()})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Normal VAE #####\n",
      "\n",
      "##### Product Space Hyper-spherical VAE #####\n",
      "[1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\ntk\\lib\\site-packages\\torch\\distributions\\distribution.py:45: UserWarning: <class 'hyperspherical_vae.distributions.hyperspherical_uniform.HypersphericalUniform'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(f'{self.__class__} does not define `arg_constraints`. ' +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'recon loss': -23.196541244279658, 'KL': 2.7919460088569235, 'ELBO': -25.988487253136583}\n",
      "##### Product Space Hyper-spherical VAE #####\n",
      "[3, 2]\n",
      "{'recon loss': -12.681391839058168, 'KL': 3.6873143132206234, 'ELBO': -16.36870615227879}\n",
      "##### Product Space Hyper-spherical VAE #####\n",
      "[10, 10, 10, 10]\n",
      "{'recon loss': -21.91185671046709, 'KL': 7.335589235686695, 'ELBO': -29.24744594615378}\n",
      "##### Product Space Hyper-spherical VAE #####\n",
      "[20, 10, 6, 1]\n",
      "{'recon loss': -20.7835997067965, 'KL': 6.546653262891863, 'ELBO': -27.330252969688363}\n",
      "##### Product Space Hyper-spherical VAE #####\n",
      "[15, 10, 4, 3, 2, 1]\n",
      "{'recon loss': -31.932040699658263, 'KL': 10.44656860674231, 'ELBO': -42.37860930640058}\n",
      "##### Product Space Hyper-spherical VAE #####\n",
      "[20, 20]\n",
      "{'recon loss': -11.908658065511926, 'KL': 2.566489924073334, 'ELBO': -14.475147989585261}\n"
     ]
    }
   ],
   "source": [
    "# hidden dimension and dimension of latent space\n",
    "H_DIM = 128\n",
    "Z_DIM = 2\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "X = torch.tensor(adata.X)\n",
    "X = X.float()\n",
    "n_gene = X.numpy().shape[1]\n",
    "\n",
    "# normal VAE\n",
    "#modelN = ModelVAE(n_gene=n_gene, z_dim=Z_DIM, encoder_layer=None, decoder_layer=None, distribution='normal', x=X)\n",
    "# print(modelN.parameters)\n",
    "# optimizerN = optim.Adam(modelN.parameters(), lr=1e-3)\n",
    "\n",
    "print('##### Normal VAE #####')\n",
    "\n",
    "n_epochs = 20\n",
    "# training for 1 epoch\n",
    "# for i in range(0, n_epochs):\n",
    "# train(modelN, optimizerN)\n",
    "\n",
    "# test\n",
    "# test(modelN, optimizerN)\n",
    "\n",
    "print()\n",
    "\n",
    "save_file_names = {\"v1\": [1, 1],\n",
    "                   \"v2\": [3, 2],\n",
    "                   \"v3\": [10, 10, 10, 10],\n",
    "                   \"v4\": [20, 10, 6, 1],\n",
    "                   \"v5\": [15, 10, 4, 3, 2, 1],\n",
    "                   \"v6\": [20, 20]\n",
    "                   }\n",
    "# for z_dim in [2, 5, 10, 20]:\n",
    "#     for Z_DIMS in [[z_dim, z_dim], [z_dim, z_dim, z_dim], [z_dim, z_dim, z_dim, z_dim, z_dim]]:\n",
    "# for Z_DIMS in [[1, 1],\n",
    "               # [3, 2],\n",
    "               # [9, 9, 9, 9],\n",
    "               # [10, 10, 10, 10],\n",
    "               # [20, 10, 6, 1],\n",
    "               # [15, 10, 4, 3, 2, 1],\n",
    "               # [20, 20]]:\n",
    "# save_file_names = {}\n",
    "for version in save_file_names:\n",
    "    Z_DIMS = save_file_names[version]\n",
    "    # Z_DIMS = [2, 2]\n",
    "    # hyper-spherical  VAE\n",
    "    encoder_layer = None\n",
    "    decoder_layer = None\n",
    "    learning_rate = 1e-5\n",
    "    use_l2_regularization = False\n",
    "    modelS = ProductSpaceVAE(n_gene=n_gene, z_dims=[z + 1 for z in Z_DIMS], encoder_layer=encoder_layer, decoder_layer=decoder_layer,\n",
    "                             distribution='vmf', device='cuda').to('cuda')\n",
    "    # print(modelS.parameters)\n",
    "    # optimizerS = optim.SGD(modelS.parameters(), lr=1e-5)\n",
    "    optimizerS = optim.Adam(modelS.parameters(), lr=learning_rate)\n",
    "\n",
    "    print('##### Product Space Hyper-spherical VAE #####')\n",
    "\n",
    "    s_epochs = 10\n",
    "    # training for 1 epoch\n",
    "    print(Z_DIMS)\n",
    "    for i in range(0, s_epochs):\n",
    "        train(modelS, optimizerS)\n",
    "    save_model(f\"product_svae_{data}_{version}\")\n",
    "    # test\n",
    "    test(modelS, optimizerS)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "def save_model(model_filename):\n",
    "  torch.save({\n",
    "      \"Z_DIMS\": Z_DIMS,\n",
    "      \"encoder_layer\": encoder_layer,\n",
    "      \"decoder_layer\": decoder_layer,\n",
    "      \"s_epochs\": s_epochs,\n",
    "      \"learning_rate\": learning_rate,\n",
    "      \"use_l2_regularization\": False,\n",
    "      \"model_state_dict\": modelS.state_dict(),\n",
    "      \"optimizer_state_dict\": optimizerS.state_dict()\n",
    "  },\n",
    "  model_filename)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "saved_model = \"product_svae_b_cells_v1\"\n",
    "if saved_model:\n",
    "  checkpoint = torch.load(saved_model)\n",
    "  n_gene = 1196\n",
    "  Z_DIMS = checkpoint[\"Z_DIMS\"]\n",
    "  encoder_layer = checkpoint[\"encoder_layer\"]\n",
    "  decoder_layer = checkpoint[\"decoder_layer\"]\n",
    "  learning_rate = checkpoint[\"learning_rate\"]\n",
    "\n",
    "  modelS = ProductSpaceVAE(n_gene=n_gene, z_dims=[z + 1 for z in Z_DIMS], encoder_layer=encoder_layer, decoder_layer=decoder_layer,\n",
    "                          distribution='vmf', device='cuda').to('cuda')\n",
    "  optimizerS = optim.Adam(modelS.parameters(), lr=learning_rate)\n",
    "\n",
    "  modelS.load_state_dict(checkpoint['model_state_dict'])\n",
    "  optimizerS.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "  test(modelS, optimizerS)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5pU77s4PcESE",
    "outputId": "cca7243d-6d28-47c4-865c-2fc8c04fb36e"
   },
   "execution_count": 61,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (128x1816 and 1196x128)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[61], line 17\u001B[0m\n\u001B[0;32m     14\u001B[0m modelS\u001B[38;5;241m.\u001B[39mload_state_dict(checkpoint[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel_state_dict\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     15\u001B[0m optimizerS\u001B[38;5;241m.\u001B[39mload_state_dict(checkpoint[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moptimizer_state_dict\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m---> 17\u001B[0m \u001B[43mtest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodelS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizerS\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[58], line 14\u001B[0m, in \u001B[0;36mtest\u001B[1;34m(model, optimizer, device)\u001B[0m\n\u001B[0;32m      8\u001B[0m     x \u001B[38;5;241m=\u001B[39m X[i:, :]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# dynamic binarization\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# x_mb = (x_mb > torch.distributions.Uniform(0, 1).sample(x_mb.shape)).float()\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# _, (q_z, p_z), _, x_mb_ = model(x_mb.reshape(-1, 784))\u001B[39;00m\n\u001B[1;32m---> 14\u001B[0m (q_zs, p_zs), z, mu_, sigma_square_ \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# print_['recon loss'].append(float(nn.BCEWithLogitsLoss(reduction='none')(x_mb_,\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# x_mb.reshape(-1, 784)).sum(-1).mean().data))\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# library_size = torch.sum(x, dim=1, keepdim=True)\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# mu_ = mu_* library_size\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# print_['recon loss'].append(float(torch.mean(log_likelihood_nb(x, mu_, sigma_square_)).data))\u001B[39;00m\n\u001B[0;32m     21\u001B[0m print_[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrecon loss\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mfloat\u001B[39m(torch\u001B[38;5;241m.\u001B[39mmean(log_likelihood_student(x, mu_, sigma_square_, df\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5.0\u001B[39m))\u001B[38;5;241m.\u001B[39mdata))\n",
      "File \u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\ntk\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn[54], line 230\u001B[0m, in \u001B[0;36mProductSpaceVAE.forward\u001B[1;34m(self, x, n)\u001B[0m\n\u001B[0;32m    228\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, n\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m--> 230\u001B[0m     z_means, z_vars \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    231\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misnan(z_means[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;241m.\u001B[39msum() \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misnan(z_vars[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;241m.\u001B[39msum() \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    232\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m), \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[54], line 98\u001B[0m, in \u001B[0;36mProductSpaceVAE.encode\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     87\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mencode\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m     88\u001B[0m     \u001B[38;5;66;03m# x = self.encoder_mlp(x)\u001B[39;00m\n\u001B[0;32m     89\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     95\u001B[0m \n\u001B[0;32m     96\u001B[0m     \u001B[38;5;66;03m# regularizer = torch.\u001B[39;00m\n\u001B[1;32m---> 98\u001B[0m     h \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder_layer\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     99\u001B[0m     h \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactivation(h)\n\u001B[0;32m    100\u001B[0m     \u001B[38;5;66;03m# Add in batch normalization here\u001B[39;00m\n\u001B[0;32m    101\u001B[0m     \u001B[38;5;66;03m# h = self.encoder_batch_norm[0](h)\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\ntk\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\ntk\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (128x1816 and 1196x128)"
     ]
    }
   ]
  }
 ]
}