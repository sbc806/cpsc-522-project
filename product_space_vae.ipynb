{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qATAyIXprzdx",
        "outputId": "1cb1c37f-80e8-4bdc-a72a-7cfa8142e460"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cpsc-522-project'...\n",
            "remote: Enumerating objects: 54, done.\u001b[K\n",
            "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 54 (delta 16), reused 28 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (54/54), 1.65 MiB | 3.09 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/sbc806/cpsc-522-project.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys as sys\n",
        "\n",
        "sys.path.append(\"cpsc-522-project\")"
      ],
      "metadata": {
        "id": "jSogYSRK1UpO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scanpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCtv8Kp51bdF",
        "outputId": "8f732c3a-4f4a-4dcc-9dc1-47f8a79ee4d1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scanpy\n",
            "  Downloading scanpy-1.9.3-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.9/dist-packages (from scanpy) (1.10.1)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.9/dist-packages (from scanpy) (0.5.3)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.9/dist-packages (from scanpy) (1.22.4)\n",
            "Collecting session-info\n",
            "  Downloading session_info-1.0.0.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib>=3.4 in /usr/local/lib/python3.9/dist-packages (from scanpy) (3.7.1)\n",
            "Collecting umap-learn>=0.3.10\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.9/dist-packages (from scanpy) (1.5.3)\n",
            "Requirement already satisfied: h5py>=3 in /usr/local/lib/python3.9/dist-packages (from scanpy) (3.8.0)\n",
            "Requirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.9/dist-packages (from scanpy) (3.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.9/dist-packages (from scanpy) (0.12.2)\n",
            "Requirement already satisfied: statsmodels>=0.10.0rc2 in /usr/local/lib/python3.9/dist-packages (from scanpy) (0.13.5)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.9/dist-packages (from scanpy) (8.3.1)\n",
            "Collecting anndata>=0.7.4\n",
            "  Downloading anndata-0.9.1-py3-none-any.whl (102 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numba>=0.41.0 in /usr/local/lib/python3.9/dist-packages (from scanpy) (0.56.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.9/dist-packages (from scanpy) (1.2.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from scanpy) (1.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from scanpy) (23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from scanpy) (4.65.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.4->scanpy) (1.0.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.4->scanpy) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.4->scanpy) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.4->scanpy) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.4->scanpy) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.4->scanpy) (4.39.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.4->scanpy) (2.8.2)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.4->scanpy) (5.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba>=0.41.0->scanpy) (67.6.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.41.0->scanpy) (0.39.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0->scanpy) (2022.7.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.22->scanpy) (3.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from patsy->scanpy) (1.16.0)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.8.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting stdlib_list\n",
            "  Downloading stdlib_list-0.8.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib>=3.4->scanpy) (3.15.0)\n",
            "Building wheels for collected packages: umap-learn, session-info, pynndescent\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82830 sha256=05356e7830d3b9989df9c334f6f79e3deab9ed4085870cd827782756919d589a\n",
            "  Stored in directory: /root/.cache/pip/wheels/f4/3e/1c/596d0a463d17475af648688443fa4846fef624d1390339e7e9\n",
            "  Building wheel for session-info (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for session-info: filename=session_info-1.0.0-py3-none-any.whl size=8042 sha256=9f4ea888dcf2db60f2edfdfe430a65913b0bc0591b40e87a4eb716c172090f50\n",
            "  Stored in directory: /root/.cache/pip/wheels/d4/fc/2e/00ca60bac7954b84907efd41baa9b4853500eaeec4228410c6\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.8-py3-none-any.whl size=55509 sha256=af98f0aae744b7ae777e26431d0b1eea1a68c077ea6ae44887789ff2d2383159\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/89/cc/59ab91ef5b21dc2ab3635528d7d227f49dfc9169905dcb959d\n",
            "Successfully built umap-learn session-info pynndescent\n",
            "Installing collected packages: stdlib_list, session-info, pynndescent, anndata, umap-learn, scanpy\n",
            "Successfully installed anndata-0.9.1 pynndescent-0.5.8 scanpy-1.9.3 session-info-1.0.0 stdlib_list-0.8.0 umap-learn-0.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install bbknn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOPwKn5w1dmV",
        "outputId": "fc7a5cec-1617-46c4-9ca0-28e9964e183f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bbknn\n",
            "  Downloading bbknn-1.5.1-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.9/dist-packages (from bbknn) (0.29.34)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from bbknn) (1.10.1)\n",
            "Requirement already satisfied: pynndescent in /usr/local/lib/python3.9/dist-packages (from bbknn) (0.5.8)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from bbknn) (1.2.2)\n",
            "Collecting annoy\n",
            "  Downloading annoy-1.17.2.tar.gz (647 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.4/647.4 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.9/dist-packages (from bbknn) (0.5.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from bbknn) (23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from bbknn) (1.22.4)\n",
            "Requirement already satisfied: llvmlite>=0.30 in /usr/local/lib/python3.9/dist-packages (from pynndescent->bbknn) (0.39.1)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.9/dist-packages (from pynndescent->bbknn) (0.56.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.9/dist-packages (from pynndescent->bbknn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->bbknn) (3.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from umap-learn->bbknn) (4.65.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba>=0.51.2->pynndescent->bbknn) (67.6.1)\n",
            "Building wheels for collected packages: annoy\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.2-cp39-cp39-linux_x86_64.whl size=581038 sha256=b0f90994a030cda9e705add59715162bd5ff98427bcf757668bd798ffadebb69\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/2e/e4/f3ae385c375b87982a2a70055061d4a6330ef4f60817e717e3\n",
            "Successfully built annoy\n",
            "Installing collected packages: annoy, bbknn\n",
            "Successfully installed annoy-1.17.2 bbknn-1.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os as os\n",
        "import scanpy as sc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from collections import defaultdict\n",
        "\n",
        "from hyperspherical_vae.distributions import VonMisesFisher\n",
        "from hyperspherical_vae.distributions import HypersphericalUniform"
      ],
      "metadata": {
        "id": "AGicNh_U1f-d"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc.settings.verbosity = 3  # verbosity: errors (0), warnings (1), info (2), hints (3)\n",
        "sc.logging.print_header()\n",
        "sc.settings.set_figure_params(dpi=80, facecolor='white')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gvlvkqgd1iGv",
        "outputId": "fd7c95df-b67a-4c64-e57d-6584f3dc0d57"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.22.4 scipy==1.10.1 pandas==1.5.3 scikit-learn==1.2.2 statsmodels==0.13.5 pynndescent==0.5.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(filename):\n",
        "    adata = sc.read_h5ad(filename)\n",
        "    return adata"
      ],
      "metadata": {
        "id": "JiFu2bQd1kNH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"./data\"\n",
        "# file_path = \"./drive/MyDrive/immune_cell_dataset/\"\n",
        "# data can be myeloid, b_cells, or t_cells\n",
        "data = \"b_cells\"\n",
        "file_path = os.path.join(file_path, data)\n",
        "if data == \"myeloid\":\n",
        "    filename = \"myeloid.h5ad\"\n",
        "elif data == \"b_cells\":\n",
        "    filename = \"b-cells.h5ad\"\n",
        "else:\n",
        "    filename = \"t-cells.h5ad\"\n",
        "# file_path = os.path.join(file_path, filename)\n",
        "\n",
        "adata = read_data(file_path)"
      ],
      "metadata": {
        "id": "ZzQOprlg1m72"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preprocessing"
      ],
      "metadata": {
        "id": "s14Hcy2p1oF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data already normalized and log-transformed\n",
        "# Select highly variable genes\n",
        "sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5)\n",
        "adata = adata[:, adata.var.highly_variable]\n",
        "print(adata.X.shape)\n",
        "# Scale to unit variance and zero mean\n",
        "sc.pp.scale(adata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZO8T9wN-1nS-",
        "outputId": "b5727735-1376-4152-8af4-0971c360dd1c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "extracting highly variable genes\n",
            "    finished (0:00:02)\n",
            "--> added\n",
            "    'highly_variable', boolean vector (adata.var)\n",
            "    'means', float vector (adata.var)\n",
            "    'dispersions', float vector (adata.var)\n",
            "    'dispersions_norm', float vector (adata.var)\n",
            "(54934, 1196)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/scanpy/preprocessing/_simple.py:843: UserWarning: Received a view of an AnnData. Making a copy.\n",
            "  view_to_actual(adata)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "... as `zero_center=True`, sparse input is densified and may lead to large memory consumption\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get rid of batch effects"
      ],
      "metadata": {
        "id": "PKoeVP4H2SBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bbknn\n",
        "\n",
        "bbknn.ridge_regression(adata, batch_key=['Chemistry'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TljSmi5V2Uwl",
        "outputId": "783deeb5-1c10-4f4e-edc3-8f45d309d78e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "computing ridge regression\n",
            "\tfinished: `.X` now features regression residuals\n",
            "\t`.layers['X_explained']` stores the expression explained by the technical effect (0:00:01)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "NSZQvriP45KH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# noinspection PyUnresolvedReferences\n",
        "# noinspection PyCallingNonCallable\n",
        "class ProductSpaceVAE(torch.nn.Module):\n",
        "\n",
        "    # def __init__(self, h_dims, z_dims, input_size=[1, 28, 28], input_type = 'binary', distribution='normal',\n",
        "    # r=None, encode_type='mlp', decode_type='mlp', device='cpu', flags=None):\n",
        "    def __init__(self, z_dims, n_gene, distribution='normal',\n",
        "                 r=None, encoder_layer=None, decoder_layer=None, activation=F.relu,\n",
        "                 device='cpu', flags=None):\n",
        "        \"\"\"\n",
        "        ModelVAE initializer\n",
        "        # :param in_dim: dimension of input\n",
        "        :param n_gene: dimension of input\n",
        "        # :param h_dims: dimension of the hidden layers, list\n",
        "        :param z_dims: dimensions of the latent representation, list\n",
        "        :param distribution: string either `normal` or `vmf`, indicates which distribution to use\n",
        "        :param r: radii scalars, list\n",
        "        :encoder_layer: a list with the units of each layer for the encoder\n",
        "        :decoder_layer: a list with units of each layer for the decoder\n",
        "        :param device: device to use\n",
        "        \"\"\"\n",
        "        super(ProductSpaceVAE, self).__init__()\n",
        "\n",
        "        self.flags = flags\n",
        "        self.name = 'productspace'\n",
        "        self.epochs, self.num_restarts = 0, 0\n",
        "        # self.input_size, self.distribution, self.device = input_size, distribution, device\n",
        "        self.n_gene, self.distribution, self.device = n_gene, distribution, device\n",
        "        # self.encode_type, self.decode_type = encode_type, decode_type\n",
        "        self.activation = activation\n",
        "\n",
        "        self.z_dims = np.sort(np.asarray(z_dims))\n",
        "        self.z_unique, self.z_counts = np.unique(self.z_dims, return_counts=True)\n",
        "        self.z_u_idx = [np.where(self.z_dims == u)[0] for u in self.z_unique]\n",
        "\n",
        "        self.r = torch.ones(len(z_dims), device=device) if r is None else r  # not used yet\n",
        "\n",
        "        # self.encoder, self.fc_means, self.fc_vars = create_encoder(input_size, input_type, self.z_dims, h_dims,\n",
        "        # distribution, encode_type, flags)\n",
        "        if encoder_layer is None:\n",
        "            encoder_layer = [128, 64, 32]\n",
        "        if decoder_layer is None:\n",
        "            decoder_layer = [32, 128]\n",
        "\n",
        "        # Output of the encoder\n",
        "        h_last = encoder_layer[-1]\n",
        "        self.fc_means = nn.ModuleList([nn.Linear(h_last, z_dim) for z_dim in z_dims])\n",
        "        self.fc_vars = nn.ModuleList([nn.Sequential(nn.Linear(h_last, (1 if distribution == 'vmf' else z_dim)),\n",
        "                                                    nn.Softplus(), nn.Hardtanh(min_val=0.01, max_val=7.))\n",
        "                                      for z_dim in z_dims])\n",
        "        # Output of decoder\n",
        "        self.mu_layer = nn.Linear(decoder_layer[-1], n_gene)\n",
        "        self.var_layer = nn.Linear(decoder_layer[-1], n_gene)\n",
        "\n",
        "        # self.decoder = create_decoder(input_size, input_type, z_dims, h_dims, decode_type)\n",
        "\n",
        "        # Create layers for encoder\n",
        "        self.encoder_layer = [nn.Linear(n_gene, encoder_layer[0])]\n",
        "        # BatchNorm\n",
        "        # self.encoder_batch_norm = [nn.BatchNorm1d(encoder_layer[0])]\n",
        "        for i in range(1, len(encoder_layer)):\n",
        "            self.encoder_layer.append(nn.Linear(encoder_layer[i - 1], encoder_layer[i]))\n",
        "            # BatchNorm\n",
        "            # self.encoder_batch_norm.append(nn.BatchNorm1d(encoder_layer[i]))\n",
        "        self.encoder_layer = nn.ModuleList(self.encoder_layer)\n",
        "        # BatchNorm\n",
        "        # self.encoder_batch_norm = nn.ModuleList(self.encoder_batch_norm)\n",
        "\n",
        "        # self.decoder_layer = [nn.Linear(z_dim, decoder_layer[0])]\n",
        "        self.decoder_layer = [nn.Linear(sum(z_dims), decoder_layer[0])]\n",
        "        # BatchNorm\n",
        "        # self.decoder_batch_norm = [nn.BatchNorm1d(decoder_layer[0])]\n",
        "        for i in range(1, len(decoder_layer)):\n",
        "            self.decoder_layer.append(nn.Linear(decoder_layer[i - 1], decoder_layer[i]))\n",
        "            # BatchNorm\n",
        "            # self.decoder_batch_norm.append(nn.BatchNorm1d(decoder_layer[i]))\n",
        "        self.decoder_layer = nn.ModuleList(self.decoder_layer)\n",
        "        # BatchNorm\n",
        "        # self.decoder_batch_norm = nn.ModuleList(self.decoder_batch_norm)\n",
        "\n",
        "    @staticmethod\n",
        "    def _print(x):\n",
        "        print('\\n')\n",
        "        print(x)\n",
        "        print('\\n')\n",
        "\n",
        "    def encode(self, x):\n",
        "        # x = self.encoder_mlp(x)\n",
        "\n",
        "        # if self.encode_type == 'cnn':\n",
        "        # x = x.reshape(x.size(0), *self.input_size)\n",
        "\n",
        "        # h = self.encoder(x)\n",
        "        # h = h.view(h.size(0), -1)\n",
        "\n",
        "        # regularizer = torch.\n",
        "\n",
        "        h = self.encoder_layer[0](x)\n",
        "        h = self.activation(h)\n",
        "        # Add in batch normalization here\n",
        "        # h = self.encoder_batch_norm[0](h)\n",
        "        for i in range(1, len(self.encoder_layer)):\n",
        "            h = self.encoder_layer[i](h)\n",
        "            h = self.activation(h)\n",
        "            # Add in batch normalization here\n",
        "            # h = self.encoder_batch_norm[i](h)\n",
        "\n",
        "        if self.distribution == 'normal':\n",
        "            # compute means and stds of the normal distributions\n",
        "            z_means = [f(h) for f in self.fc_means]\n",
        "            z_vars = [f(h) for f in self.fc_vars]\n",
        "        elif self.distribution == 'vmf':\n",
        "            # compute means and concentrations of the von Mises-Fishers\n",
        "            z_means_unnormalized = [f(h) for f in self.fc_means]\n",
        "            z_means = [zmu / zmu.norm(dim=-1, keepdim=True) for zmu in z_means_unnormalized]\n",
        "            z_vars = [(f(h) + 1.) for f in self.fc_vars]  # the `+ 1` prevents collapsing behaviors\n",
        "        else:\n",
        "            raise NotImplemented\n",
        "\n",
        "        return z_means, z_vars\n",
        "\n",
        "    def decode(self, z):\n",
        "        # if self.decode_type == 'cnn':\n",
        "        # z = z.view(z.size(0), sum(self.z_dims), 1, 1)\n",
        "\n",
        "        # x_recon = self.decoder(z)\n",
        "\n",
        "        # return x_recon.view(x_recon.size(0), -1)\n",
        "        # l2 regularization goes here\n",
        "        h = self.decoder_layer[0](z)\n",
        "        h = self.activation(h)\n",
        "        # Add in batch normalization here\n",
        "        # h = self.decoder_batch_norm[0](h)\n",
        "        for i in range(1, len(self.decoder_layer)):\n",
        "            h = self.decoder_layer[i](h)\n",
        "            h = self.activation(h)\n",
        "            # Add in batch normalization here\n",
        "            # h = self.decoder_batch_norm[i](h)\n",
        "\n",
        "        mu = self.mu_layer(h)\n",
        "        sigma_square = F.softplus(self.var_layer(h))\n",
        "        sigma_sqare = torch.clip(sigma_square, 1e-6, 1e10)\n",
        "        return mu, sigma_square\n",
        "\n",
        "    def reparameterize(self, z_means, z_vars):\n",
        "\n",
        "        # since z is sorted, we take the min index, and the max index, to slice the list of z_means, z_vars\n",
        "        # this is done to not have convert to numpy array\n",
        "        gather_zvs = [(torch.cat(z_means[min(u_idx):max(u_idx) + 1], 0),\n",
        "                       torch.cat(z_vars[min(u_idx):max(u_idx) + 1], 0))\n",
        "                      for u_idx in self.z_u_idx]\n",
        "\n",
        "        if self.distribution == 'normal':\n",
        "            # for each pair of z_mean, z_var, we make a distribution (sampling) object\n",
        "            q_zs_sample = [torch.distributions.normal.Normal(z_mean, z_var) for (z_mean, z_var) in gather_zvs]\n",
        "\n",
        "            q_zs = [torch.distributions.normal.Normal(z_mean, z_var) for z_mean, z_var in zip(z_means, z_vars)]\n",
        "            p_zs = [torch.distributions.normal.Normal(torch.zeros_like(z_mean), torch.ones_like(z_var)) for\n",
        "                    z_mean, z_var in zip(z_means, z_vars)]\n",
        "\n",
        "        elif self.distribution == 'vmf':\n",
        "            # for each pair of z_mean, z_var, we make a distribution (sampling) object\n",
        "            q_zs_sample = [VonMisesFisher(z_mean, z_var) for (z_mean, z_var) in gather_zvs]\n",
        "\n",
        "            q_zs = [VonMisesFisher(z_mean, z_var) for z_mean, z_var in zip(z_means, z_vars)]\n",
        "            p_zs = [HypersphericalUniform(z_dim - 1, device=self.device) for z_dim in self.z_dims]\n",
        "        else:\n",
        "            raise NotImplemented\n",
        "\n",
        "        return q_zs, p_zs, q_zs_sample\n",
        "\n",
        "    # def loss(self, q_zs, p_zs, x_mb, x_mb_recon):\n",
        "    def kl_divergence(self, q_zs, p_zs):\n",
        "        # if self.flags.loss_function == 'bce':\n",
        "        # lf = nn.BCEWithLogitsLoss(reduction='none')\n",
        "        # elif self.flags.loss_function == 'mse':\n",
        "        # lf = nn.MSELoss(reduction='none')\n",
        "        # else:\n",
        "        # raise NotImplemented\n",
        "        # loss_recon = lf(x_mb_recon, x_mb.reshape(x_mb.size(0), -1)).sum(-1).mean()\n",
        "\n",
        "        if self.distribution == 'normal':\n",
        "            loss_kl = torch.stack([torch.distributions.kl.kl_divergence(q_z, p_z).sum(-1) for\n",
        "                                   q_z, p_z in zip(q_zs, p_zs)], dim=-1).sum(-1).mean()\n",
        "        elif self.distribution == 'vmf':\n",
        "            loss_kl = torch.stack([torch.distributions.kl.kl_divergence(q_z, p_z) for\n",
        "                                   q_z, p_z in zip(q_zs, p_zs)], dim=-1).sum(-1).mean()\n",
        "        else:\n",
        "            raise NotImplemented\n",
        "\n",
        "        # return loss_recon, loss_kl, None\n",
        "        return loss_kl\n",
        "\n",
        "    # def log_likelihood(self, x, n=10):\n",
        "    # \"\"\"\n",
        "    # :param x: e.g. MNIST data flattened\n",
        "    # :param n: number of MC samples\n",
        "    # :return: MC estimate of log-likelihood\n",
        "    # \"\"\"\n",
        "\n",
        "    # z_means, z_vars = self.encode(x.reshape(x.size(0), -1))\n",
        "    # q_zs, p_zs, _, = self.reparameterize(z_means, z_vars)\n",
        "    # z_parts = [q_z.rsample(torch.Size([n])) for q_z in q_zs]\n",
        "    # z = torch.cat(z_parts, dim=-1).reshape(n*x.size(0), -1)\n",
        "\n",
        "    # x_mb_recon = self.decode(z)\n",
        "    # x_mb_recon = x_mb_recon.reshape(n, x.size(0), -1)\n",
        "\n",
        "    # if self.flags.loss_function == 'bce':\n",
        "    # lf = nn.BCEWithLogitsLoss(reduction='none')\n",
        "    # elif self.flags.loss_function == 'mse':\n",
        "    # lf = nn.MSELoss(reduction='none')\n",
        "    # else:\n",
        "    # raise NotImplemented\n",
        "    # log_p_x_z = -lf(x_mb_recon, x.reshape(x.size(0), -1).repeat((n, 1, 1))).sum(-1)\n",
        "\n",
        "    # if self.distribution == 'normal':\n",
        "    # log_p_z = torch.stack([p_z.log_prob(z__).sum(-1) for p_z, z__ in zip(p_zs, z_parts)], dim=-1).sum(-1)\n",
        "    # log_q_z_x = torch.stack([q_z.log_prob(z__).sum(-1) for q_z, z__ in zip(q_zs, z_parts)], dim=-1).sum(-1)\n",
        "    # elif self.distribution == 'vmf':\n",
        "    # log_p_z = torch.stack([p_z.log_prob(z__) for p_z, z__ in zip(p_zs, z_parts)], dim=-1).sum(-1)\n",
        "    # log_q_z_x = torch.stack([q_z.log_prob(z__) for q_z, z__ in zip(q_zs, z_parts)], dim=-1).sum(-1)\n",
        "    # else:\n",
        "    # raise NotImplementedError\n",
        "\n",
        "    # return ((log_p_x_z + log_p_z.to(self.device) - log_q_z_x).t().logsumexp(-1) - np.log(n)).mean()\n",
        "\n",
        "    def forward(self, x, n=None):\n",
        "\n",
        "        z_means, z_vars = self.encode(x)\n",
        "        if torch.isnan(z_means[0]).sum() > 0 or torch.isnan(z_vars[0]).sum() > 0:\n",
        "            return (None, None), None, None\n",
        "\n",
        "        q_zs, p_zs, q_zs_sample = self.reparameterize(z_means, z_vars)\n",
        "\n",
        "        # sample z1, z2, .., zk and concatenate\n",
        "        # z = torch.cat([q_z.rsample(torch.Size() if n is None else torch.Size([n])) for q_z in q_zs], dim=-1)  # slow\n",
        "        z = torch.cat([torch.cat(torch.chunk(q_z.rsample(), int(c), dim=0), dim=-1)\n",
        "                       for q_z, c in zip(q_zs_sample, self.z_counts)], dim=-1)\n",
        "        # z_parts = list(torch.split(z, tuple(self.z_unique.repeat(self.z_counts)), -1))\n",
        "        mu, sigma_square = self.decode(z)\n",
        "\n",
        "        return (q_zs, p_zs), z, mu, sigma_square"
      ],
      "metadata": {
        "id": "Dvdz0ayP45pY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_likelihood(model, x, n=10):\n",
        "    z_mean, z_var = model.encode(x)\n",
        "    q_zs, p_zs, q_zs_sample = model.reparameterize(z_mean, z_var)\n",
        "    z = q_z.rsample(torch.Size([n]))\n",
        "    # Need to add a line here like z = torch.cat() in the forward_pass(self, x, n=None)\n",
        "    mu_, sigma_square_ = model.decode(z)\n",
        "\n",
        "    # In scPhere used tf.reduce_mean()\n",
        "    return torch.mean(log_likelihood_student(x, mu_, sigma_square_, df=2.0))"
      ],
      "metadata": {
        "id": "6t-GQ03l4SqG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_likelihood_student(x, mu, sigma_square, df=2.0):\n",
        "    sigma = torch.sqrt(sigma_square)\n",
        "\n",
        "    dist = torch.distributions.studentT.StudentT(df=df,\n",
        "                                                 loc=mu,\n",
        "                                                 scale=sigma)\n",
        "\n",
        "    # return tf.reduce_sum(dist.log_prob(x), reduction_indices=1)\n",
        "    return torch.mean(dist.log_prob(x), dim=1)"
      ],
      "metadata": {
        "id": "dnIJ3oiI4c2-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, device='cuda'):\n",
        "    # for i, (x_mb, y_mb) in enumerate(train_loader):\n",
        "    for i in range(0, X.shape[0], 128):\n",
        "        # for x in X:\n",
        "        if i + 128 < X.shape[0]:\n",
        "            x = X[i: i + 128, :].to(device)\n",
        "        else:\n",
        "            x = X[i:, :].to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # dynamic binarization\n",
        "        # x_mb = (x_mb > torch.distributions.Uniform(0, 1).sample(x_mb.shape)).float()\n",
        "\n",
        "        # _, (q_z, p_z), _, x_mb_ = model(x_mb.reshape(-1, 784))\n",
        "        (q_zs, p_zs), z, mu, sigma_square = model(x)\n",
        "\n",
        "        # loss_recon = nn.BCEWithLogitsLoss(reduction='none')(x_reconstructed, x).sum(-1).mean()\n",
        "        # loss_recon = log_likelihood(x, z_mean, z_var)\n",
        "        # library_size = torch.sum(x, dim=1, keepdim=True)\n",
        "        # mu = mu * library_size\n",
        "        # loss_recon = log_likelihood_nb(\n",
        "        # x,\n",
        "        # mu,\n",
        "        # sigma_square,\n",
        "        # ).sum(-1).mean()\n",
        "        # loss_recon = torch.mean(log_likelihood_nb(x, mu, sigma_square, eps=1e-10))\n",
        "        loss_recon = torch.mean(log_likelihood_student(x, mu, sigma_square, df=5.0))\n",
        "\n",
        "        if model.distribution == 'normal':\n",
        "            loss_KL = torch.distributions.kl.kl_divergence(q_z, p_z).sum(-1).mean()\n",
        "        elif model.distribution == 'vmf':\n",
        "            # loss_KL = torch.distributions.kl.kl_divergence(q_z, p_z).mean()\n",
        "            loss_KL = model.kl_divergence(q_zs, p_zs)\n",
        "        else:\n",
        "            raise NotImplemented\n",
        "        loss = loss_recon - loss_KL\n",
        "        # loss = loss_recon + loss_KL\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "-2yBsLq75LrG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, optimizer, device='cuda'):\n",
        "    print_ = defaultdict(list)\n",
        "    # for x_mb, y_mb in test_loader:\n",
        "    for i in range(0, X.shape[0], 128):\n",
        "        if i + 128 < X.shape[0]:\n",
        "            x = X[i:i + 128, :].to(device)\n",
        "        else:\n",
        "            x = X[i:, :].to(device)\n",
        "\n",
        "        # dynamic binarization\n",
        "        # x_mb = (x_mb > torch.distributions.Uniform(0, 1).sample(x_mb.shape)).float()\n",
        "\n",
        "        # _, (q_z, p_z), _, x_mb_ = model(x_mb.reshape(-1, 784))\n",
        "        (q_zs, p_zs), z, mu_, sigma_square_ = model(x)\n",
        "\n",
        "        # print_['recon loss'].append(float(nn.BCEWithLogitsLoss(reduction='none')(x_mb_,\n",
        "        # x_mb.reshape(-1, 784)).sum(-1).mean().data))\n",
        "        # library_size = torch.sum(x, dim=1, keepdim=True)\n",
        "        # mu_ = mu_* library_size\n",
        "        # print_['recon loss'].append(float(torch.mean(log_likelihood_nb(x, mu_, sigma_square_)).data))\n",
        "        print_['recon loss'].append(float(torch.mean(log_likelihood_student(x, mu_, sigma_square_, df=5.0)).data))\n",
        "\n",
        "        if model.distribution == 'normal':\n",
        "            print_['KL'].append(float(torch.distributions.kl.kl_divergence(q_z, p_z).sum(-1).mean().data))\n",
        "        elif model.distribution == 'vmf':\n",
        "            # print_['KL'].append(float(torch.distributions.kl.kl_divergence(q_z, p_z).mean().data))\n",
        "            print_['KL'].append(float(model.kl_divergence(q_zs, p_zs).mean().data))\n",
        "        else:\n",
        "            raise NotImplemented\n",
        "\n",
        "        print_['ELBO'].append(print_['recon loss'][-1] - print_['KL'][-1])\n",
        "        # print_['LL'].append(float(log_likelihood(model, x).data))\n",
        "\n",
        "    print({k: np.mean(v) for k, v in print_.items()})"
      ],
      "metadata": {
        "id": "NPXYNCJE5OOG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using model"
      ],
      "metadata": {
        "id": "eMADcDot4jjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hidden dimension and dimension of latent space\n",
        "H_DIM = 128\n",
        "Z_DIM = 2\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "X = torch.tensor(adata.X)\n",
        "X = X.float()\n",
        "n_gene = X.numpy().shape[1]\n",
        "\n",
        "# normal VAE\n",
        "#modelN = ModelVAE(n_gene=n_gene, z_dim=Z_DIM, encoder_layer=None, decoder_layer=None, distribution='normal', x=X)\n",
        "# print(modelN.parameters)\n",
        "# optimizerN = optim.Adam(modelN.parameters(), lr=1e-3)\n",
        "\n",
        "print('##### Normal VAE #####')\n",
        "\n",
        "n_epochs = 20\n",
        "# training for 1 epoch\n",
        "# for i in range(0, n_epochs):\n",
        "# train(modelN, optimizerN)\n",
        "\n",
        "# test\n",
        "# test(modelN, optimizerN)\n",
        "\n",
        "print()\n",
        "\n",
        "save_file_names = {\"v1\": [1, 1],\n",
        "                   \"v2\": [3, 2],\n",
        "                   \"v3\": [10, 10, 10, 10],\n",
        "                   \"v4\": [20, 10, 6, 1],\n",
        "                   \"v5\": [15, 10, 4, 3, 2, 1],\n",
        "                   \"v6\": [20, 20]\n",
        "                   }\n",
        "# for z_dim in [2, 5, 10, 20]:\n",
        "#     for Z_DIMS in [[z_dim, z_dim], [z_dim, z_dim, z_dim], [z_dim, z_dim, z_dim, z_dim, z_dim]]:\n",
        "# for Z_DIMS in [[1, 1],\n",
        "               # [3, 2],\n",
        "               # [9, 9, 9, 9],\n",
        "               # [10, 10, 10, 10],\n",
        "               # [20, 10, 6, 1],\n",
        "               # [15, 10, 4, 3, 2, 1],\n",
        "               # [20, 20]]:\n",
        "save_file_names = {}\n",
        "for version in save_file_names:\n",
        "    Z_DIMS = save_file_names[version]\n",
        "    # Z_DIMS = [2, 2]\n",
        "    # hyper-spherical  VAE\n",
        "    encoder_layer = None\n",
        "    decoder_layer = None\n",
        "    learning_rate = 1e-5\n",
        "    use_l2_regularization = False\n",
        "    modelS = ProductSpaceVAE(n_gene=n_gene, z_dims=[z + 1 for z in Z_DIMS], encoder_layer=encoder_layer, decoder_layer=decoder_layer,\n",
        "                             distribution='vmf', device='cuda').to('cuda')\n",
        "    # print(modelS.parameters)\n",
        "    # optimizerS = optim.SGD(modelS.parameters(), lr=1e-5)\n",
        "    optimizerS = optim.Adam(modelS.parameters(), lr=learning_rate)\n",
        "\n",
        "    print('##### Product Space Hyper-spherical VAE #####')\n",
        "\n",
        "    s_epochs = 10\n",
        "    # training for 1 epoch\n",
        "    print(Z_DIMS)\n",
        "    for i in range(0, s_epochs):\n",
        "        train(modelS, optimizerS)\n",
        "    save_model(f\"product_svae_{data}_{version}\")\n",
        "    # test\n",
        "    test(modelS, optimizerS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywDEiOEn2Wsl",
        "outputId": "93f68a4d-b3b2-4b4d-bf23-b71ccd1e15d6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##### Normal VAE #####\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model_filename):\n",
        "  torch.save({\n",
        "      \"Z_DIMS\": Z_DIMS,\n",
        "      \"encoder_layer\": encoder_layer,\n",
        "      \"decoder_layer\": decoder_layer,\n",
        "      \"s_epochs\": s_epochs,\n",
        "      \"learning_rate\": learning_rate,\n",
        "      \"use_l2_regularization\": False,\n",
        "      \"model_state_dict\": modelS.state_dict(),\n",
        "      \"optimizer_state_dict\": optimizerS.state_dict()\n",
        "  },\n",
        "  model_filename)"
      ],
      "metadata": {
        "id": "ZlwHuLCK37Pe"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model = \"product_svae_b_cells_v1\"\n",
        "if saved_model:\n",
        "  checkpoint = torch.load(saved_model)\n",
        "  n_gene = 1196\n",
        "  Z_DIMS = checkpoint[\"Z_DIMS\"]\n",
        "  encoder_layer = checkpoint[\"encoder_layer\"]\n",
        "  decoder_layer = checkpoint[\"decoder_layer\"]\n",
        "  learning_rate = checkpoint[\"learning_rate\"]\n",
        "\n",
        "  modelS = ProductSpaceVAE(n_gene=n_gene, z_dims=[z + 1 for z in Z_DIMS], encoder_layer=encoder_layer, decoder_layer=decoder_layer,\n",
        "                          distribution='vmf', device='cuda').to('cuda')\n",
        "  optimizerS = optim.Adam(modelS.parameters(), lr=learning_rate)\n",
        "\n",
        "  modelS.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizerS.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "  test(modelS, optimizerS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pU77s4PcESE",
        "outputId": "cca7243d-6d28-47c4-865c-2fc8c04fb36e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/distributions/distribution.py:51: UserWarning: <class 'hyperspherical_vae.distributions.hyperspherical_uniform.HypersphericalUniform'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
            "  warnings.warn(f'{self.__class__} does not define `arg_constraints`. ' +\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'recon loss': -1.0840655720511148, 'KL': 0.9558083916144016, 'ELBO': -2.0398739636655163}\n"
          ]
        }
      ]
    }
  ]
}