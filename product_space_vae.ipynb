{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "owNGEbJgljYm",
    "outputId": "22afce09-37fd-4250-f4d4-123581232259",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: Too many arguments.\n",
      "\n",
      "usage: git clone [<options>] [--] <repo> [<dir>]\n",
      "\n",
      "    -v, --verbose         be more verbose\n",
      "    -q, --quiet           be more quiet\n",
      "    --progress            force progress reporting\n",
      "    --reject-shallow      don't clone shallow repository\n",
      "    -n, --no-checkout     don't create a checkout\n",
      "    --bare                create a bare repository\n",
      "    --mirror              create a mirror repository (implies bare)\n",
      "    -l, --local           to clone from a local repository\n",
      "    --no-hardlinks        don't use local hardlinks, always copy\n",
      "    -s, --shared          setup as shared repository\n",
      "    --recurse-submodules[=<pathspec>]\n",
      "                          initialize submodules in the clone\n",
      "    --recursive ...       alias of --recurse-submodules\n",
      "    -j, --jobs <n>        number of submodules cloned in parallel\n",
      "    --template <template-directory>\n",
      "                          directory from which templates will be used\n",
      "    --reference <repo>    reference repository\n",
      "    --reference-if-able <repo>\n",
      "                          reference repository\n",
      "    --dissociate          use --reference only while cloning\n",
      "    -o, --origin <name>   use <name> instead of 'origin' to track upstream\n",
      "    -b, --branch <branch>\n",
      "                          checkout <branch> instead of the remote's HEAD\n",
      "    -u, --upload-pack <path>\n",
      "                          path to git-upload-pack on the remote\n",
      "    --depth <depth>       create a shallow clone of that depth\n",
      "    --shallow-since <time>\n",
      "                          create a shallow clone since a specific time\n",
      "    --shallow-exclude <revision>\n",
      "                          deepen history of shallow clone, excluding rev\n",
      "    --single-branch       clone only one branch, HEAD or --branch\n",
      "    --no-tags             don't clone any tags, and make later fetches not to follow them\n",
      "    --shallow-submodules  any cloned submodules will be shallow\n",
      "    --separate-git-dir <gitdir>\n",
      "                          separate git dir from working tree\n",
      "    -c, --config <key=value>\n",
      "                          set config inside the new repository\n",
      "    --server-option <server-specific>\n",
      "                          option to transmit\n",
      "    -4, --ipv4            use IPv4 addresses only\n",
      "    -6, --ipv6            use IPv6 addresses only\n",
      "    --filter <args>       object filtering\n",
      "    --remote-submodules   any cloned submodules will use their remote-tracking branch\n",
      "    --sparse              initialize sparse-checkout file to include only files at root\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git clone https: // github.com / sbc806 / cpsc-522-project.git"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import sys as sys\n",
    "\n",
    "sys.path.append(\"cpsc-522-project\")"
   ],
   "metadata": {
    "id": "mQhy7PRRtIxt",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install scanpy"
   ],
   "metadata": {
    "id": "D9oQgpMRJTN3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scanpy in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (1.9.3)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (1.2.0)\n",
      "Requirement already satisfied: matplotlib>=3.4 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (3.6.3)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (4.64.1)\n",
      "Requirement already satisfied: natsort in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (8.3.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (1.23.5)\n",
      "Requirement already satisfied: numba>=0.41.0 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (0.56.4)\n",
      "Requirement already satisfied: session-info in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (1.0.0)\n",
      "Requirement already satisfied: networkx>=2.3 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (3.1)\n",
      "Requirement already satisfied: umap-learn>=0.3.10 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (0.5.3)\n",
      "Requirement already satisfied: patsy in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (0.5.3)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.4 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (1.10.0)\n",
      "Requirement already satisfied: pandas>=1.0 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (1.5.3)\n",
      "Requirement already satisfied: statsmodels>=0.10.0rc2 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (0.13.5)\n",
      "Requirement already satisfied: anndata>=0.7.4 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (0.9.1)\n",
      "Requirement already satisfied: seaborn in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (0.12.2)\n",
      "Requirement already satisfied: h5py>=3 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (3.8.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scanpy) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from matplotlib>=3.4->scanpy) (9.4.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from matplotlib>=3.4->scanpy) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from matplotlib>=3.4->scanpy) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from matplotlib>=3.4->scanpy) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from matplotlib>=3.4->scanpy) (1.0.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from matplotlib>=3.4->scanpy) (4.38.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from matplotlib>=3.4->scanpy) (0.11.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from numba>=0.41.0->scanpy) (66.0.0)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from numba>=0.41.0->scanpy) (0.39.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from pandas>=1.0->scanpy) (2022.7.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scikit-learn>=0.22->scanpy) (3.1.0)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from patsy->scanpy) (1.16.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from umap-learn>=0.3.10->scanpy) (0.5.8)\n",
      "Requirement already satisfied: stdlib-list in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from session-info->scanpy) (0.8.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from tqdm->scanpy) (0.4.6)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip3 install bbknn"
   ],
   "metadata": {
    "id": "ojvzqSBcQOLf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bbknn in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from bbknn) (1.10.0)\n",
      "Requirement already satisfied: umap-learn in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from bbknn) (0.5.3)\n",
      "Requirement already satisfied: annoy in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from bbknn) (1.17.2)\n",
      "Requirement already satisfied: Cython in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from bbknn) (0.29.33)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from bbknn) (23.0)\n",
      "Requirement already satisfied: pynndescent in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from bbknn) (0.5.8)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from bbknn) (1.2.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from bbknn) (1.23.5)\n",
      "Requirement already satisfied: numba>=0.51.2 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from pynndescent->bbknn) (0.56.4)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from pynndescent->bbknn) (1.2.0)\n",
      "Requirement already satisfied: llvmlite>=0.30 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from pynndescent->bbknn) (0.39.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from scikit-learn->bbknn) (3.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from umap-learn->bbknn) (4.64.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from numba>=0.51.2->pynndescent->bbknn) (66.0.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\ntk\\lib\\site-packages (from tqdm->umap-learn->bbknn) (0.4.6)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "import numpy as np\n",
    "import os as os\n",
    "import scanpy as sc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from collections import defaultdict\n",
    "\n",
    "from hyperspherical_vae.distributions import VonMisesFisher\n",
    "from hyperspherical_vae.distributions import HypersphericalUniform"
   ],
   "metadata": {
    "id": "FjGjPkwMyfYY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sc.settings.verbosity = 3  # verbosity: errors (0), warnings (1), info (2), hints (3)\n",
    "sc.logging.print_header()\n",
    "sc.settings.set_figure_params(dpi=80, facecolor='white')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VCKS_0RyccEi",
    "outputId": "36b77963-5a9b-4e92-8bca-1f38a2d4484c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.23.5 scipy==1.10.0 pandas==1.5.3 scikit-learn==1.2.0 statsmodels==0.13.5 pynndescent==0.5.8\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def read_data(filename):\n",
    "    adata = sc.read_h5ad(filename)\n",
    "    return adata"
   ],
   "metadata": {
    "id": "dBlNLreAJcXY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "file_path = \"./data\"\n",
    "# data can be myeloid, b_cells, or t_cells\n",
    "data = \"b_cells\"\n",
    "# file_path = os.path.join(file_path, data)\n",
    "if data == \"myeloid\":\n",
    "    filename = \"myeloid.h5ad\"\n",
    "elif data == \"b_cells\":\n",
    "    filename = \"b-cells.h5ad\"\n",
    "else:\n",
    "    filename = \"t-cells.h5ad\"\n",
    "file_path = os.path.join(file_path, filename)\n",
    "\n",
    "adata = read_data(file_path)"
   ],
   "metadata": {
    "id": "4JXdxVLVyELl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data preprocessing"
   ],
   "metadata": {
    "id": "n-oHwvK3VT4U",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Data already normalized and log-transformed\n",
    "# Select highly variable genes\n",
    "sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5)\n",
    "adata = adata[:, adata.var.highly_variable]\n",
    "print(adata.X.shape)\n",
    "# Scale to unit variance and zero mean\n",
    "sc.pp.scale(adata)"
   ],
   "metadata": {
    "id": "LzRxSvlnRGa-",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7ff53718-99c0-450d-efe6-b291f9570a8e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting highly variable genes\n",
      "    finished (0:00:01)\n",
      "--> added\n",
      "    'highly_variable', boolean vector (adata.var)\n",
      "    'means', float vector (adata.var)\n",
      "    'dispersions', float vector (adata.var)\n",
      "    'dispersions_norm', float vector (adata.var)\n",
      "(54934, 1196)\n",
      "... as `zero_center=True`, sparse input is densified and may lead to large memory consumption\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\ntk\\lib\\site-packages\\scanpy\\preprocessing\\_simple.py:843: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get rid of batch effects"
   ],
   "metadata": {
    "id": "OrOcAtxcOGlP",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import bbknn\n",
    "\n",
    "bbknn.ridge_regression(adata, batch_key=['Chemistry'])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vHftXwxDQZEW",
    "outputId": "b2532bd8-82da-4794-e9d6-830137251069",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing ridge regression\n",
      "\tfinished: `.X` now features regression residuals\n",
      "\t`.layers['X_explained']` stores the expression explained by the technical effect (0:00:00)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model"
   ],
   "metadata": {
    "id": "2vkBIZBr4O2L",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# noinspection PyUnresolvedReferences\n",
    "# noinspection PyCallingNonCallable\n",
    "class ProductSpaceVAE(torch.nn.Module):\n",
    "\n",
    "    # def __init__(self, h_dims, z_dims, input_size=[1, 28, 28], input_type = 'binary', distribution='normal',\n",
    "    # r=None, encode_type='mlp', decode_type='mlp', device='cpu', flags=None):\n",
    "    def __init__(self, z_dims, n_gene, distribution='normal',\n",
    "                 r=None, encoder_layer=None, decoder_layer=None, activation=F.relu,\n",
    "                 device='cpu', flags=None):\n",
    "        \"\"\"\n",
    "        ModelVAE initializer\n",
    "        # :param in_dim: dimension of input\n",
    "        :param n_gene: dimension of input\n",
    "        # :param h_dims: dimension of the hidden layers, list\n",
    "        :param z_dims: dimensions of the latent representation, list\n",
    "        :param distribution: string either `normal` or `vmf`, indicates which distribution to use\n",
    "        :param r: radii scalars, list\n",
    "        :encoder_layer: a list with the units of each layer for the encoder\n",
    "        :decoder_layer: a list with units of each layer for the decoder\n",
    "        :param device: device to use\n",
    "        \"\"\"\n",
    "        super(ProductSpaceVAE, self).__init__()\n",
    "\n",
    "        self.flags = flags\n",
    "        self.name = 'productspace'\n",
    "        self.epochs, self.num_restarts = 0, 0\n",
    "        # self.input_size, self.distribution, self.device = input_size, distribution, device\n",
    "        self.n_gene, self.distribution, self.device = n_gene, distribution, device\n",
    "        # self.encode_type, self.decode_type = encode_type, decode_type\n",
    "        self.activation = activation\n",
    "\n",
    "        self.z_dims = np.sort(np.asarray(z_dims))\n",
    "        self.z_unique, self.z_counts = np.unique(self.z_dims, return_counts=True)\n",
    "        self.z_u_idx = [np.where(self.z_dims == u)[0] for u in self.z_unique]\n",
    "\n",
    "        self.r = torch.ones(len(z_dims), device=device) if r is None else r  # not used yet\n",
    "\n",
    "        # self.encoder, self.fc_means, self.fc_vars = create_encoder(input_size, input_type, self.z_dims, h_dims,\n",
    "        # distribution, encode_type, flags)\n",
    "        if encoder_layer is None:\n",
    "            encoder_layer = [128, 64, 32]\n",
    "        if decoder_layer is None:\n",
    "            decoder_layer = [32, 128]\n",
    "\n",
    "        # Output of the encoder\n",
    "        h_last = encoder_layer[-1]\n",
    "        self.fc_means = nn.ModuleList([nn.Linear(h_last, z_dim) for z_dim in z_dims])\n",
    "        self.fc_vars = nn.ModuleList([nn.Sequential(nn.Linear(h_last, (1 if distribution == 'vmf' else z_dim)),\n",
    "                                                    nn.Softplus(), nn.Hardtanh(min_val=0.01, max_val=7.))\n",
    "                                      for z_dim in z_dims])\n",
    "        # Output of decoder\n",
    "        self.mu_layer = nn.Linear(decoder_layer[-1], n_gene)\n",
    "        self.var_layer = nn.Linear(decoder_layer[-1], n_gene)\n",
    "\n",
    "        # self.decoder = create_decoder(input_size, input_type, z_dims, h_dims, decode_type)\n",
    "\n",
    "        # Create layers for encoder\n",
    "        self.encoder_layer = [nn.Linear(n_gene, encoder_layer[0])]\n",
    "        # BatchNorm\n",
    "        # self.encoder_batch_norm = [nn.BatchNorm1d(encoder_layer[0])]\n",
    "        for i in range(1, len(encoder_layer)):\n",
    "            self.encoder_layer.append(nn.Linear(encoder_layer[i - 1], encoder_layer[i]))\n",
    "            # BatchNorm\n",
    "            # self.encoder_batch_norm.append(nn.BatchNorm1d(encoder_layer[i]))\n",
    "        self.encoder_layer = nn.ModuleList(self.encoder_layer)\n",
    "        # BatchNorm\n",
    "        # self.encoder_batch_norm = nn.ModuleList(self.encoder_batch_norm)\n",
    "\n",
    "        # self.decoder_layer = [nn.Linear(z_dim, decoder_layer[0])]\n",
    "        self.decoder_layer = [nn.Linear(sum(z_dims), decoder_layer[0])]\n",
    "        # BatchNorm\n",
    "        # self.decoder_batch_norm = [nn.BatchNorm1d(decoder_layer[0])]\n",
    "        for i in range(1, len(decoder_layer)):\n",
    "            self.decoder_layer.append(nn.Linear(decoder_layer[i - 1], decoder_layer[i]))\n",
    "            # BatchNorm\n",
    "            # self.decoder_batch_norm.append(nn.BatchNorm1d(decoder_layer[i]))\n",
    "        self.decoder_layer = nn.ModuleList(self.decoder_layer)\n",
    "        # BatchNorm\n",
    "        # self.decoder_batch_norm = nn.ModuleList(self.decoder_batch_norm)\n",
    "\n",
    "    @staticmethod\n",
    "    def _print(x):\n",
    "        print('\\n')\n",
    "        print(x)\n",
    "        print('\\n')\n",
    "\n",
    "    def encode(self, x):\n",
    "        # x = self.encoder_mlp(x)\n",
    "\n",
    "        # if self.encode_type == 'cnn':\n",
    "        # x = x.reshape(x.size(0), *self.input_size)\n",
    "\n",
    "        # h = self.encoder(x)\n",
    "        # h = h.view(h.size(0), -1)\n",
    "\n",
    "        # regularizer = torch.\n",
    "\n",
    "        h = self.encoder_layer[0](x)\n",
    "        h = self.activation(h)\n",
    "        # Add in batch normalization here\n",
    "        # h = self.encoder_batch_norm[0](h)\n",
    "        for i in range(1, len(self.encoder_layer)):\n",
    "            h = self.encoder_layer[i](h)\n",
    "            h = self.activation(h)\n",
    "            # Add in batch normalization here\n",
    "            # h = self.encoder_batch_norm[i](h)\n",
    "\n",
    "        if self.distribution == 'normal':\n",
    "            # compute means and stds of the normal distributions\n",
    "            z_means = [f(h) for f in self.fc_means]\n",
    "            z_vars = [f(h) for f in self.fc_vars]\n",
    "        elif self.distribution == 'vmf':\n",
    "            # compute means and concentrations of the von Mises-Fishers\n",
    "            z_means_unnormalized = [f(h) for f in self.fc_means]\n",
    "            z_means = [zmu / zmu.norm(dim=-1, keepdim=True) for zmu in z_means_unnormalized]\n",
    "            z_vars = [(f(h) + 1.) for f in self.fc_vars]  # the `+ 1` prevents collapsing behaviors\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "\n",
    "        return z_means, z_vars\n",
    "\n",
    "    def decode(self, z):\n",
    "        # if self.decode_type == 'cnn':\n",
    "        # z = z.view(z.size(0), sum(self.z_dims), 1, 1)\n",
    "\n",
    "        # x_recon = self.decoder(z)\n",
    "\n",
    "        # return x_recon.view(x_recon.size(0), -1)\n",
    "        # l2 regularization goes here\n",
    "        h = self.decoder_layer[0](z)\n",
    "        h = self.activation(h)\n",
    "        # Add in batch normalization here\n",
    "        # h = self.decoder_batch_norm[0](h)\n",
    "        for i in range(1, len(self.decoder_layer)):\n",
    "            h = self.decoder_layer[i](h)\n",
    "            h = self.activation(h)\n",
    "            # Add in batch normalization here\n",
    "            # h = self.decoder_batch_norm[i](h)\n",
    "\n",
    "        mu = self.mu_layer(h)\n",
    "        sigma_square = F.softplus(self.var_layer(h))\n",
    "        sigma_sqare = torch.clip(sigma_square, 1e-6, 1e10)\n",
    "        return mu, sigma_square\n",
    "\n",
    "    def reparameterize(self, z_means, z_vars):\n",
    "\n",
    "        # since z is sorted, we take the min index, and the max index, to slice the list of z_means, z_vars\n",
    "        # this is done to not have convert to numpy array\n",
    "        gather_zvs = [(torch.cat(z_means[min(u_idx):max(u_idx) + 1], 0),\n",
    "                       torch.cat(z_vars[min(u_idx):max(u_idx) + 1], 0))\n",
    "                      for u_idx in self.z_u_idx]\n",
    "\n",
    "        if self.distribution == 'normal':\n",
    "            # for each pair of z_mean, z_var, we make a distribution (sampling) object\n",
    "            q_zs_sample = [torch.distributions.normal.Normal(z_mean, z_var) for (z_mean, z_var) in gather_zvs]\n",
    "\n",
    "            q_zs = [torch.distributions.normal.Normal(z_mean, z_var) for z_mean, z_var in zip(z_means, z_vars)]\n",
    "            p_zs = [torch.distributions.normal.Normal(torch.zeros_like(z_mean), torch.ones_like(z_var)) for\n",
    "                    z_mean, z_var in zip(z_means, z_vars)]\n",
    "\n",
    "        elif self.distribution == 'vmf':\n",
    "            # for each pair of z_mean, z_var, we make a distribution (sampling) object\n",
    "            q_zs_sample = [VonMisesFisher(z_mean, z_var) for (z_mean, z_var) in gather_zvs]\n",
    "\n",
    "            q_zs = [VonMisesFisher(z_mean, z_var) for z_mean, z_var in zip(z_means, z_vars)]\n",
    "            p_zs = [HypersphericalUniform(z_dim - 1, device=self.device) for z_dim in self.z_dims]\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "\n",
    "        return q_zs, p_zs, q_zs_sample\n",
    "\n",
    "    # def loss(self, q_zs, p_zs, x_mb, x_mb_recon):\n",
    "    def kl_divergence(self, q_zs, p_zs):\n",
    "        # if self.flags.loss_function == 'bce':\n",
    "        # lf = nn.BCEWithLogitsLoss(reduction='none')\n",
    "        # elif self.flags.loss_function == 'mse':\n",
    "        # lf = nn.MSELoss(reduction='none')\n",
    "        # else:\n",
    "        # raise NotImplemented\n",
    "        # loss_recon = lf(x_mb_recon, x_mb.reshape(x_mb.size(0), -1)).sum(-1).mean()\n",
    "\n",
    "        if self.distribution == 'normal':\n",
    "            loss_kl = torch.stack([torch.distributions.kl.kl_divergence(q_z, p_z).sum(-1) for\n",
    "                                   q_z, p_z in zip(q_zs, p_zs)], dim=-1).sum(-1).mean()\n",
    "        elif self.distribution == 'vmf':\n",
    "            loss_kl = torch.stack([torch.distributions.kl.kl_divergence(q_z, p_z) for\n",
    "                                   q_z, p_z in zip(q_zs, p_zs)], dim=-1).sum(-1).mean()\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "\n",
    "        # return loss_recon, loss_kl, None\n",
    "        return loss_kl\n",
    "\n",
    "    # def log_likelihood(self, x, n=10):\n",
    "    # \"\"\"\n",
    "    # :param x: e.g. MNIST data flattened\n",
    "    # :param n: number of MC samples\n",
    "    # :return: MC estimate of log-likelihood\n",
    "    # \"\"\"\n",
    "\n",
    "    # z_means, z_vars = self.encode(x.reshape(x.size(0), -1))\n",
    "    # q_zs, p_zs, _, = self.reparameterize(z_means, z_vars)\n",
    "    # z_parts = [q_z.rsample(torch.Size([n])) for q_z in q_zs]\n",
    "    # z = torch.cat(z_parts, dim=-1).reshape(n*x.size(0), -1)\n",
    "\n",
    "    # x_mb_recon = self.decode(z)\n",
    "    # x_mb_recon = x_mb_recon.reshape(n, x.size(0), -1)\n",
    "\n",
    "    # if self.flags.loss_function == 'bce':\n",
    "    # lf = nn.BCEWithLogitsLoss(reduction='none')\n",
    "    # elif self.flags.loss_function == 'mse':\n",
    "    # lf = nn.MSELoss(reduction='none')\n",
    "    # else:\n",
    "    # raise NotImplemented\n",
    "    # log_p_x_z = -lf(x_mb_recon, x.reshape(x.size(0), -1).repeat((n, 1, 1))).sum(-1)\n",
    "\n",
    "    # if self.distribution == 'normal':\n",
    "    # log_p_z = torch.stack([p_z.log_prob(z__).sum(-1) for p_z, z__ in zip(p_zs, z_parts)], dim=-1).sum(-1)\n",
    "    # log_q_z_x = torch.stack([q_z.log_prob(z__).sum(-1) for q_z, z__ in zip(q_zs, z_parts)], dim=-1).sum(-1)\n",
    "    # elif self.distribution == 'vmf':\n",
    "    # log_p_z = torch.stack([p_z.log_prob(z__) for p_z, z__ in zip(p_zs, z_parts)], dim=-1).sum(-1)\n",
    "    # log_q_z_x = torch.stack([q_z.log_prob(z__) for q_z, z__ in zip(q_zs, z_parts)], dim=-1).sum(-1)\n",
    "    # else:\n",
    "    # raise NotImplementedError\n",
    "\n",
    "    # return ((log_p_x_z + log_p_z.to(self.device) - log_q_z_x).t().logsumexp(-1) - np.log(n)).mean()\n",
    "\n",
    "    def forward(self, x, n=None):\n",
    "\n",
    "        z_means, z_vars = self.encode(x)\n",
    "        if torch.isnan(z_means[0]).sum() > 0 or torch.isnan(z_vars[0]).sum() > 0:\n",
    "            return (None, None), None, None\n",
    "\n",
    "        q_zs, p_zs, q_zs_sample = self.reparameterize(z_means, z_vars)\n",
    "\n",
    "        # sample z1, z2, .., zk and concatenate\n",
    "        # z = torch.cat([q_z.rsample(torch.Size() if n is None else torch.Size([n])) for q_z in q_zs], dim=-1)  # slow\n",
    "        z = torch.cat([torch.cat(torch.chunk(q_z.rsample(), int(c), dim=0), dim=-1)\n",
    "                       for q_z, c in zip(q_zs_sample, self.z_counts)], dim=-1)\n",
    "        # z_parts = list(torch.split(z, tuple(self.z_unique.repeat(self.z_counts)), -1))\n",
    "        mu, sigma_square = self.decode(z)\n",
    "\n",
    "        return (q_zs, p_zs), z, mu, sigma_square"
   ],
   "metadata": {
    "id": "uHS8RmiT5nxj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "im-cMSpAuIPk",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def log_likelihood(model, x, n=10):\n",
    "    z_mean, z_var = model.encode(x)\n",
    "    q_zs, p_zs, q_zs_sample = model.reparameterize(z_mean, z_var)\n",
    "    z = q_z.rsample(torch.Size([n]))\n",
    "    # Need to add a line here like z = torch.cat() in the forward_pass(self, x, n=None)\n",
    "    mu_, sigma_square_ = model.decode(z)\n",
    "\n",
    "    # In scPhere used tf.reduce_mean()\n",
    "    return torch.mean(log_likelihood_student(x, mu_, sigma_square_, df=2.0))"
   ],
   "metadata": {
    "id": "iCU-PhnG1-1P",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def log_likelihood_student(x, mu, sigma_square, df=2.0):\n",
    "    sigma = torch.sqrt(sigma_square)\n",
    "\n",
    "    dist = torch.distributions.studentT.StudentT(df=df,\n",
    "                                                 loc=mu,\n",
    "                                                 scale=sigma)\n",
    "\n",
    "    # return tf.reduce_sum(dist.log_prob(x), reduction_indices=1)\n",
    "    return torch.mean(dist.log_prob(x), dim=1)"
   ],
   "metadata": {
    "id": "Sf9tZtDwzsNT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train(model, optimizer, device='cuda'):\n",
    "    # for i, (x_mb, y_mb) in enumerate(train_loader):\n",
    "    for i in range(0, X.shape[0], 128):\n",
    "        # for x in X:\n",
    "        if i + 128 < X.shape[0]:\n",
    "            x = X[i: i + 128, :].to(device)\n",
    "        else:\n",
    "            x = X[i:, :].to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # dynamic binarization\n",
    "        # x_mb = (x_mb > torch.distributions.Uniform(0, 1).sample(x_mb.shape)).float()\n",
    "\n",
    "        # _, (q_z, p_z), _, x_mb_ = model(x_mb.reshape(-1, 784))\n",
    "        (q_zs, p_zs), z, mu, sigma_square = model(x)\n",
    "\n",
    "        # loss_recon = nn.BCEWithLogitsLoss(reduction='none')(x_reconstructed, x).sum(-1).mean()\n",
    "        # loss_recon = log_likelihood(x, z_mean, z_var)\n",
    "        # library_size = torch.sum(x, dim=1, keepdim=True)\n",
    "        # mu = mu * library_size\n",
    "        # loss_recon = log_likelihood_nb(\n",
    "        # x,\n",
    "        # mu,\n",
    "        # sigma_square,\n",
    "        # ).sum(-1).mean()\n",
    "        # loss_recon = torch.mean(log_likelihood_nb(x, mu, sigma_square, eps=1e-10))\n",
    "        loss_recon = torch.mean(log_likelihood_student(x, mu, sigma_square, df=5.0))\n",
    "\n",
    "        if model.distribution == 'normal':\n",
    "            loss_KL = torch.distributions.kl.kl_divergence(q_z, p_z).sum(-1).mean()\n",
    "        elif model.distribution == 'vmf':\n",
    "            # loss_KL = torch.distributions.kl.kl_divergence(q_z, p_z).mean()\n",
    "            loss_KL = model.kl_divergence(q_zs, p_zs)\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "        loss = loss_recon - loss_KL\n",
    "        # loss = loss_recon + loss_KL\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ],
   "metadata": {
    "id": "eF8UqZkH2vnm",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def test(model, optimizer, device='cuda'):\n",
    "    print_ = defaultdict(list)\n",
    "    # for x_mb, y_mb in test_loader:\n",
    "    for i in range(0, X.shape[0], 128):\n",
    "        if i + 128 < X.shape[0]:\n",
    "            x = X[i:i + 128, :].to(device)\n",
    "        else:\n",
    "            x = X[i:, :].to(device)\n",
    "\n",
    "        # dynamic binarization\n",
    "        # x_mb = (x_mb > torch.distributions.Uniform(0, 1).sample(x_mb.shape)).float()\n",
    "\n",
    "        # _, (q_z, p_z), _, x_mb_ = model(x_mb.reshape(-1, 784))\n",
    "        (q_zs, p_zs), z, mu_, sigma_square_ = model(x)\n",
    "\n",
    "        # print_['recon loss'].append(float(nn.BCEWithLogitsLoss(reduction='none')(x_mb_,\n",
    "        # x_mb.reshape(-1, 784)).sum(-1).mean().data))\n",
    "        # library_size = torch.sum(x, dim=1, keepdim=True)\n",
    "        # mu_ = mu_* library_size\n",
    "        # print_['recon loss'].append(float(torch.mean(log_likelihood_nb(x, mu_, sigma_square_)).data))\n",
    "        print_['recon loss'].append(float(torch.mean(log_likelihood_student(x, mu_, sigma_square_, df=5.0)).data))\n",
    "\n",
    "        if model.distribution == 'normal':\n",
    "            print_['KL'].append(float(torch.distributions.kl.kl_divergence(q_z, p_z).sum(-1).mean().data))\n",
    "        elif model.distribution == 'vmf':\n",
    "            # print_['KL'].append(float(torch.distributions.kl.kl_divergence(q_z, p_z).mean().data))\n",
    "            print_['KL'].append(float(model.kl_divergence(q_zs, p_zs).mean().data))\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "\n",
    "        print_['ELBO'].append(print_['recon loss'][-1] - print_['KL'][-1])\n",
    "        # print_['LL'].append(float(log_likelihood(model, x).data))\n",
    "\n",
    "    print({k: np.mean(v) for k, v in print_.items()})"
   ],
   "metadata": {
    "id": "K9bJ7H6XfwBq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using model"
   ],
   "metadata": {
    "id": "NdYlhdgBBQul",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# hidden dimension and dimension of latent space\n",
    "H_DIM = 128\n",
    "Z_DIM = 2\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "X = torch.tensor(adata.X)\n",
    "X = X.float()\n",
    "n_gene = X.numpy().shape[1]\n",
    "\n",
    "# normal VAE\n",
    "#modelN = ModelVAE(n_gene=n_gene, z_dim=Z_DIM, encoder_layer=None, decoder_layer=None, distribution='normal', x=X)\n",
    "# print(modelN.parameters)\n",
    "# optimizerN = optim.Adam(modelN.parameters(), lr=1e-3)\n",
    "\n",
    "print('##### Normal VAE #####')\n",
    "\n",
    "n_epochs = 20\n",
    "# training for 1 epoch\n",
    "# for i in range(0, n_epochs):\n",
    "# train(modelN, optimizerN)\n",
    "\n",
    "# test\n",
    "# test(modelN, optimizerN)\n",
    "\n",
    "print()\n",
    "\n",
    "# for z_dim in [2, 5, 10, 20]:\n",
    "#     for Z_DIMS in [[z_dim, z_dim], [z_dim, z_dim, z_dim], [z_dim, z_dim, z_dim, z_dim, z_dim]]:\n",
    "for Z_DIMS in [[1, 1],\n",
    "               [3, 2],\n",
    "               [9, 9, 9, 9],\n",
    "               [10, 10, 10, 10],\n",
    "               [20, 10, 6, 1],\n",
    "               [15, 10, 4, 3, 2, 1],\n",
    "               [20, 20]]:\n",
    "    # Z_DIMS = [2, 2]\n",
    "    # hyper-spherical  VAE\n",
    "    modelS = ProductSpaceVAE(n_gene=n_gene, z_dims=[z + 1 for z in Z_DIMS], encoder_layer=None, decoder_layer=None,\n",
    "                             distribution='vmf', device='cuda').to('cuda')\n",
    "    # print(modelS.parameters)\n",
    "    # optimizerS = optim.SGD(modelS.parameters(), lr=1e-5)\n",
    "    optimizerS = optim.Adam(modelS.parameters(), lr=1e-5)\n",
    "\n",
    "    print('##### Product Space Hyper-spherical VAE #####')\n",
    "\n",
    "    s_epochs = 10\n",
    "    # training for 1 epoch\n",
    "    print(Z_DIMS)\n",
    "    for i in range(0, s_epochs):\n",
    "        train(modelS, optimizerS)\n",
    "\n",
    "    # test\n",
    "    test(modelS, optimizerS)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4223b509-7ca7-4c45-c1d7-3fa6209760d9",
    "id": "Ukyldi02X499",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Normal VAE #####\n",
      "\n",
      "##### Product Space Hyper-spherical VAE #####\n",
      "[1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\ntk\\lib\\site-packages\\torch\\distributions\\distribution.py:45: UserWarning: <class 'hyperspherical_vae.distributions.hyperspherical_uniform.HypersphericalUniform'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(f'{self.__class__} does not define `arg_constraints`. ' +\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 49\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28mprint\u001B[39m(Z_DIMS)\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, s_epochs):\n\u001B[1;32m---> 49\u001B[0m     \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodelS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizerS\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;66;03m# test\u001B[39;00m\n\u001B[0;32m     52\u001B[0m test(modelS, optimizerS)\n",
      "Cell \u001B[1;32mIn[14], line 39\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, optimizer, device)\u001B[0m\n\u001B[0;32m     36\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_recon \u001B[38;5;241m-\u001B[39m loss_KL\n\u001B[0;32m     37\u001B[0m \u001B[38;5;66;03m# loss = loss_recon + loss_KL\u001B[39;00m\n\u001B[1;32m---> 39\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     40\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\ntk\\lib\\site-packages\\torch\\_tensor.py:488\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    478\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    479\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    480\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    481\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    486\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    487\u001B[0m     )\n\u001B[1;32m--> 488\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    490\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\ntk\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    192\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    194\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    195\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    196\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 197\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  }
 ]
}